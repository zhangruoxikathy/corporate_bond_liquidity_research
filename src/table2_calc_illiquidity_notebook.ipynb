{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2 Measure of Illiquidity\n",
    "\n",
    "This notebook walks through illiquidity calculations based on methodology in The Illiquidity of Corporate Bonds, Bao, Pan, and Wang (2010). In the paper, calculations are based on corporate bond data from 2003-04-14 to 2009-06-30.\n",
    "\n",
    "  - In order to avoid re-running the notebook every time it changes (it changes often, even by the act of opening it) and to only rerun it if meaningful changes have been made, the build system only looks for changes in the plaintext version of the notebook. That is, the notebook is converted to a Python script via [nbconvert](https://nbconvert.readthedocs.io/en/latest/), which is often packaged with Jupyter.\n",
    "  Then, DoIt looks for changes to the Python version. If it detects a difference, then the notebook is re-run. (Note, that you could also convert to a Markdown file with \n",
    "  [JupyText](https://github.com/mwouts/jupytext). However, this package is often not packaged with Jupyter.)\n",
    "  - Since we want to use Jupyter Notebooks for exploratory reports, we want to keep fully-computed versions of the notebook (with the output intact). However, earlier I said that I strip the notebook of its output before committing to version control. Well, to keep the output, every time PyDoit runs the notebook, it outputs an HTML version of the freshly run notebook and saves that HTML report in the `output` directory. That way, you will be able to view the finished report at any time without having to open Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='purple'>Overview of Outputs\n",
    "\n",
    "#### * Table 2 Measure of Illiquidity:\n",
    "- ##### Panel A Individual Bonds (The mean and average monthly illiquidity per bond per year)\n",
    "    - Using trade-by-trade data\n",
    "    - Using daily data\n",
    "- ##### Panel B Bond Portfolio\n",
    "    - Equal-weighted: Consider a daily portfolio composed of all bonds, with equally weighted bond returns used to calculate annual illiquidity\n",
    "    - Issuance-weighted: Consider a daily portfolio composed of all bonds, with issuance weighted bond returns used to calculate annual illiquidity\n",
    "- ##### Panel C Implied by quoted bid-ask spread\n",
    "    - Mean and median monthly bond bid-ask spread per year\n",
    "\n",
    "#### * Summary Statistics of Monthly Per Bond Illiquidity Using Daily Data\n",
    "#### * Panel A and Summary Statistics Using MMN corrected data\n",
    "#### * Replicate the Tables in the Paper (2003-04-14 to 2009-06-30) \n",
    "#### * Update the Tables to the present (2003-04-14 to present)\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../assets/table2_screenshot.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "OUTPUT_DIR = config.OUTPUT_DIR\n",
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import config\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc_tools\n",
    "import load_wrds_bondret\n",
    "import load_opensource\n",
    "import data_processing as data\n",
    "import table2_calc_illiquidity as calc_illiquidity\n",
    "import table2_plot_illiquidity as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time frames used in the paper and the updated time stamp\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "start_date = '2003-04-14'\n",
    "end_date = '2009-06-30' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Clean Merged Data for Daily Illiquidity Calculation\n",
    "\n",
    "Before calculating illiquidity measures, it's essential to ensure that our corporate bond data is accurate and relevant. The `clean_merged_data` function takes care of preparing the pre-cleaned merged monthly and daily data by performing several critical cleaning steps:\n",
    "\n",
    "- Loads and merges the relevant datasets within the specified date range.\n",
    "- Removes any records with missing crucial price information and sorts the data chronologically.\n",
    "- Adjusts for trade execution dates by incorporating a time lag to identify consecutive trades for the same bond, and filters out those that do not fall within a one-week window, accounting for holidays.\n",
    "- Consolidates the cleaned data, readying it for the subsequent illiquidity analysis.\n",
    "\n",
    "This step is crucial to ensure that the subsequent calculations are based on a dataset that reflects true trading activity without distortions from missing data or trades too far apart in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_merged_data(start_date, end_date):\n",
    "    \"\"\"Load merged, pre-cleaned daily and monthly corporate bond data for a given time interval.\"\"\"\n",
    "\n",
    "    # load and merge pre-cleaned daily and monthly data\n",
    "    df_daily = load_opensource.load_daily_bond(data_dir=DATA_DIR)\n",
    "    df_bondret = load_wrds_bondret.load_bondret(data_dir=DATA_DIR)\n",
    "    merged_df = data.all_trace_data_merge(df_daily, df_bondret,\n",
    "                                          start_date = start_date, end_date = end_date)\n",
    "    merged_df = data.sample_selection(merged_df, start_date = start_date,\n",
    "                                      end_date = end_date)\n",
    "\n",
    "    # Clean data\n",
    "    merged_df = merged_df.dropna(subset=['prclean'])\n",
    "    merged_df = merged_df.sort_values(by='trd_exctn_dt')\n",
    "    merged_df['month_year'] = pd.to_datetime(merged_df['trd_exctn_dt']).dt.to_period('M') \n",
    "\n",
    "    # Lags days for day_counts\n",
    "    merged_df['trd_exctn_dt_lag'] = merged_df.groupby('cusip')['trd_exctn_dt'].shift(1)\n",
    "    dfDC = merged_df.dropna(subset=['trd_exctn_dt_lag'])\n",
    "\n",
    "    # Generate a list of U.S. holidays over this period\n",
    "    # Only include \"daily\" return if the gap between trades is less than 1-Week \n",
    "    calendar = USFederalHolidayCalendar()\n",
    "    holidays = calendar.holidays(start_date, end_date)  # 01JUL2002  # 31DEC2022\n",
    "    holiday_date_list = holidays.date.tolist()\n",
    "\n",
    "    dfDC['n']  = np.busday_count(dfDC['trd_exctn_dt_lag'].values.astype('M8[D]') , \n",
    "                                        dfDC['trd_exctn_dt'].values.astype('M8[D]'),\n",
    "                                        holidays = holiday_date_list)\n",
    "\n",
    "    df = merged_df.merge(dfDC[['cusip', 'trd_exctn_dt', 'n']],\n",
    "                         left_on = ['cusip','trd_exctn_dt'], \n",
    "                         right_on = ['cusip','trd_exctn_dt'], how = \"left\")\n",
    "    del(dfDC)\n",
    "    df = df[df.n <= 7]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_paper = calc_illiquidity.clean_merged_data(start_date, end_date)\n",
    "cleaned_df_paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_new = calc_illiquidity.clean_merged_data(end_date, today)\n",
    "cleaned_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Price Changes and Perform Additional Cleaning\n",
    "\n",
    "In this part of the analysis pipeline, we use the `calc_deltaprc` function to compute daily price changes for corporate bonds, designed to operate on cleaned and merged daily corporate bond trade data.\n",
    "\n",
    "This calculation is based on the Measure of Illiquidity on page 10 and 11 of the peper: $ \\gamma = -\\text{Cov}(p_t - p_{t-1}, p_{t+1} - p_t) $. The process involves several steps:\n",
    "- Calculation of Log Prices: Transform cleaned prices to log prices for more stable numerical properties.\n",
    "- Lagged and Lead Price Changes: Determine the price changes by computing lagged and lead log prices.\n",
    "- Restricting Returns: Ensure that calculated price changes (returns) are within the range of -100% to 100%.\n",
    "- Conversion to Percentage: Change the representation of price changes from decimal to percentage for clarity.\n",
    "- Cleaning Data: Remove entries with incomplete information to maintain the quality of the dataset.\n",
    "- Filtering by Trade Count: Exclude bonds with fewer than 10 trade observations to focus on more reliable data.\n",
    "\n",
    "This function is essential for preparing the bond price data for accurate calculation of financial metrics such as illiquidity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deltaprc(df):\n",
    "    \"\"\"Calculate delta price and delta price_lag for each daily trades with additional cleaning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate lagged and lead log prices, and corresponding delta p (percentage returns)\n",
    "    df['logprc']     = np.log(df['prclean'])\n",
    "    df['logprc_lag'] = df.groupby( 'cusip' )['logprc'].shift(1)\n",
    "    df['deltap']     = df ['logprc'] - df ['logprc_lag']\n",
    "\n",
    "    # Restrict log returns to be in the interval [1,1]\n",
    "    df['deltap'] = np.where(df['deltap'] > 1, 1, df['deltap'])\n",
    "    df['deltap'] = np.where(df['deltap'] <-1, -1, df['deltap'])\n",
    "\n",
    "    # Convert deltap to % i.e. returns in % as opposed to decimals\n",
    "    df['deltap'] = df['deltap'] * 100\n",
    "    \n",
    "    # Repeat similar process for deltap_lag\n",
    "    df['logprc_lead'] = df.groupby( 'cusip' )['logprc'].shift(-1)\n",
    "    df['deltap_lag'] = df ['logprc_lead'] - df ['logprc']\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] > 1, 1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] <-1, -1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = df['deltap_lag'] * 100\n",
    "\n",
    "    # Drop NAs in deltap, deltap_lag and bonds < 10 observations of the paired price changes\n",
    "    df_final = df.dropna(subset=['deltap', 'deltap_lag', 'prclean'])\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper = calc_illiquidity.calc_deltaprc(cleaned_df_paper)\n",
    "df_paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = calc_deltaprc(cleaned_df_new)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Panel A Individual Bond: Illiquidity Metrics Calculation Using Daily Bond Data\n",
    "\n",
    "This step involves using the `calc_annual_illiquidity_table_daily` function to calculate and summarize annual illiquidity metrics for corporate bonds. The function takes daily bond data as input and computes several statistics that capture the illiquidity of bonds on an annual basis. `create_annual_illiquidity_table` function is used as the last step in `calc_annual_illiquidity_table_daily` to generate illiquidity table with significance percentage, robust t-stat, mean and median. \n",
    "\n",
    "- Computes the illiquidity for each bond by month by taking the negative of the covariance between daily price changes (`deltap`) and their lagged values (`deltap_lag`).\n",
    "\n",
    "- Aggregated the monthly illiquidity measures to obtain annual statistics, including mean and median illiquidity.\n",
    "\n",
    "- Calculates t-statistics for the mean illiquidity of each bond and year and determines the percentage of these t-stats that are significant (>= 1.96).\n",
    "\n",
    "- Calculates robust t-stats are calculated using OLS with HAC (heteroskedasticity and autocorrelation consistent) standard errors.\n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean and median illiquidity, the percentage of significant t-statistics, and robust t-statistics for each year, as well as for the full sample period.\n",
    "\n",
    "This comprehensive illiquidity metric calculation allows us to understand the annual and overall liquidity characteristics of the corporate bond market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_illiquidity_table(Illiq_month):\n",
    "    \"\"\"Create Panel A illquidity table with cleaned monthly illiquidity data.\"\"\"\n",
    "\n",
    "    overall_illiq_mean = np.mean(Illiq_month['illiq'])\n",
    "    overall_illiq_median = Illiq_month['illiq'].median()\n",
    "\n",
    "    # Calculate t-statistics for each cusip in each year\n",
    "    Illiq_month['t stat'] = Illiq_month.groupby(['cusip', 'year'])['illiq'].transform(\n",
    "        lambda x: (x.mean() / x.sem()) if x.sem() > 0 else np.nan)\n",
    "\n",
    "    # Identify the entries with t-stat >= 1.96 and calculate the percentage of significant t-stats for each year\n",
    "    Illiq_month['significant'] = Illiq_month['t stat'] >= 1.96\n",
    "    percent_significant = Illiq_month.groupby('year')['significant'].mean() * 100\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq', 't stat'])\n",
    "    overall_percent_significant = Illiq_month['significant'].mean() * 100\n",
    "    \n",
    "    # Calculate robust t-stat for each year\n",
    "    def get_robust_t_stat(group):\n",
    "        \"\"\"Run OLS on a constant term only (mean of illiq) to get the intercept's t-stat.\"\"\"\n",
    "        X = add_constant(group['illiq'])\n",
    "        ols_result = OLS(group['illiq'], X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "\n",
    "    robust_t_stats = Illiq_month.groupby('year').apply(get_robust_t_stat)\n",
    "    \n",
    "    \n",
    "    def calculate_overall_robust_t_stat(series):\n",
    "        X = add_constant(series)\n",
    "        ols_result = OLS(series, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "    # Call the function and assign the result to overall_robust_t_stat\n",
    "    overall_robust_t_stat = calculate_overall_robust_t_stat(Illiq_month['illiq'].dropna())\n",
    "\n",
    "    # Combine the results\n",
    "    table2_daily = pd.DataFrame({\n",
    "        'Year': robust_t_stats.index,\n",
    "        'Mean illiq': Illiq_month.groupby('year')['illiq'].mean(),\n",
    "        'Median illiq': Illiq_month.groupby('year')['illiq'].median(),\n",
    "        'Per t greater 1.96': percent_significant,\n",
    "        'Robust t stat': robust_t_stats.values\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean illiq': [overall_illiq_mean],\n",
    "        'Median illiq': [overall_illiq_median],\n",
    "        'Per t greater 1.96': [overall_percent_significant],\n",
    "        'Robust t stat': [overall_robust_t_stat]\n",
    "    })\n",
    "\n",
    "    table2_daily = pd.concat([table2_daily, overall_data], ignore_index=True)\n",
    "\n",
    "    return Illiq_month, table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annual_illiquidity_table_daily(df):\n",
    "    \"\"\"Calculate illiquidity = -cov(deltap, deltap_lag) using daily data, by month.\"\"\"\n",
    "\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    Illiq_month = df.groupby(['cusip','month_year'] )[['deltap','deltap_lag']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    Illiq_month = Illiq_month.reset_index()\n",
    "    Illiq_month.columns = ['cusip','month_year','illiq']\n",
    "    Illiq_month['year'] = Illiq_month['month_year'].dt.year\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq'])\n",
    "    # Illiq_month = Illiq_month[Illiq_month['illiq'] < 2000]  # for outliers\n",
    "    Illiq_month, table2_daily = create_annual_illiquidity_table(Illiq_month)\n",
    "    \n",
    "    return Illiq_month, table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_paper, table2_daily_paper = calc_illiquidity.calc_annual_illiquidity_table_daily(df_paper)\n",
    "table2_daily_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_new, table2_daily_new = calc_illiquidity.calc_annual_illiquidity_table_daily(df_new)\n",
    "table2_daily_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Summary Statistics Compilation Using Daily Illiquidity Data\n",
    "\n",
    "This step entails utilizing the `create_summary_stats` function to compile key summary statistics that characterize daily illiquidity data for corporate bonds over different years--min, mean, median, max, 25%, 75% std monthly illiquidity per cusip and mean t-stat. This aids in understanding the distribution and central tendencies of bond illiquidity and t-statistics on an annual basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_stats(illiq_daily):\n",
    "    \"\"\"Calculate relevant summary statistics of the illiquidity daily data.\"\"\"\n",
    "    \n",
    "    summary_stats = illiq_daily.groupby('year').agg({\n",
    "    'illiq': ['min', 'mean', lambda x: x.quantile(0.25), 'median',\n",
    "              lambda x: x.quantile(0.75), 'max', 'std'],\n",
    "    't stat': 'mean'\n",
    "    })\n",
    "    summary_stats.columns = ['min illiq', 'mean illiq', 'q1 0.25', 'median',\n",
    "                             'q3 0.75', 'max illiq', 'std illiq', 'mean t stat']\n",
    "    summary_stats.reset_index(inplace=True)\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_summary_paper = calc_illiquidity.create_summary_stats(illiq_daily_paper)\n",
    "illiq_daily_summary_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_paper[illiq_daily_paper['illiq'] > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_summary_new = calc_illiquidity.create_summary_stats(illiq_daily_new)\n",
    "illiq_daily_summary_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Panel A Using MMN Corrected Daily Bond Data\n",
    "\n",
    "Now, we apply similar calculation in Step 3 and 4 using MMN corrected daily bond data. Since the MMN corrected daily bond data contains illiquidty directly, `calc_illiq_w_mmn_corrected` performs cleaning on MMN corrected data and apply `create_annual_illiquidity_table` to generate the similar Panel A (daily data) illiquidity final table, ready for comparison. We then use the in Step 4 to produce summary stats using cleaned MMN corrected daily bond data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_illiq_w_mmn_corrected(start_date, end_date, cleaned_df):\n",
    "    \"\"\"Use clean merged cusips to filter out mmn corrected monthly data to generate illiquidity table.\"\"\"\n",
    "\n",
    "    mmn  = load_opensource.load_mmn_corrected_bond(data_dir=DATA_DIR)\n",
    "    \n",
    "    # Filter out corrected data using cleaned cusips and dates\n",
    "    mmn = mmn[(mmn['date'] >= start_date) & (mmn['date'] <= end_date)]\n",
    "    unique_cusip = np.unique(cleaned_df['cusip'])\n",
    "    mmn = mmn[mmn['cusip'].isin(unique_cusip)]\n",
    "    \n",
    "    # Clean data\n",
    "    mmn['year'] = pd.to_datetime(mmn['date']).dt.to_period('Y') \n",
    "    mmn = mmn.dropna(subset=['ILLIQ'])\n",
    "    mmn['illiq'] = mmn['ILLIQ']\n",
    "    \n",
    "    mmn, table2_daily = create_annual_illiquidity_table(mmn)\n",
    "    \n",
    "    return mmn, table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_paper, table2_daily_mmn_paper = calc_illiquidity.calc_illiq_w_mmn_corrected(\n",
    "    start_date, end_date, cleaned_df_paper)\n",
    "table2_daily_mmn_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_summary_mmn_paper = calc_illiquidity.create_summary_stats(mmn_paper)\n",
    "illiq_daily_summary_mmn_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_new, table2_daily_mmn_new = calc_illiquidity.calc_illiq_w_mmn_corrected(\n",
    "    end_date, today, cleaned_df_new)\n",
    "table2_daily_mmn_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_daily_summary_mmn_new = calc_illiquidity.create_summary_stats(mmn_new)\n",
    "illiq_daily_summary_mmn_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Panel B Bond Portfolios: Portfolio-Based Annual Illiquidity Metrics Calculation\n",
    "\n",
    "The `calc_annual_illiquidity_table_portfolio` function computes the illiquidity metrics for corporate bonds by constructing equal-weighted and issuance-weighted portfolio returns on a daily basis and then calculate portfolio illiquidity on an annual basis. The function systematically processes transaction-level bond data to assess market liquidity through portfolio aggregation, offering a more holistic view of the market dynamics. \n",
    "\n",
    "- Equal-Weighted Portfolio Calculation: Creat an equal-weighted portfolio for each trading day by averaging the daily price changes (deltap) and their lagged values (deltap_lag). It then groups these daily averages by year to calculate the negative covariance between the deltap and deltap_lag to derive the illiquidity measure for each year. Additionally, a t-statistic for the mean illiquidity of the equal-weighted portfolio is computed.\n",
    "\n",
    "- Issuance-Weighted Portfolio Calculation: Each bond is calculated with its $ \\text{issuance} = \\text{offering amount} \\times \\text{principal amount} \\times \\text{offering price} / 100 / 1,000,000 $ , and all bonds deltap and deltap_lag are aggregated on a daily basis weighted by issurance. The following steps are similar to Equal-Weighted Portfolio Calculation.\n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean equal_weighted portfolio and t-stat, mean issuance-weighted portfolio illiquidity and t-stat for each year, as well as for the full sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annual_illiquidity_table_portfolio(df):\n",
    "    \"\"\"Calculate illiquidity by using equal weighted and issurance weighted portfolios for each year.\n",
    "    \"\"\"\n",
    "    # Equal weighted\n",
    "    df_ew = df.groupby('trd_exctn_dt')[['deltap', 'deltap_lag']].mean().reset_index()\n",
    "    df_ew['year'] = df_ew['trd_exctn_dt'].dt.year\n",
    "\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    Illiq_port_ew = df_ew.groupby(['year'] )[['deltap','deltap_lag']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    Illiq_port_ew = Illiq_port_ew.reset_index()\n",
    "    Illiq_port_ew.columns = ['year','Equal-weighted']\n",
    "    Illiq_port_ew = Illiq_port_ew.dropna(subset=['Equal-weighted'])\n",
    "    \n",
    "    # for full equal weighted porfolio illiquidity\n",
    "    df_ew['full'] = 1 \n",
    "    Illiq_port_ew_full = df_ew.groupby(['full'] )[['deltap','deltap_lag']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    \n",
    "    # Calculate t-stat for equal-weighted illiquidity\n",
    "    Illiq_port_ew['EW t-stat'] = Illiq_port_ew.apply(\n",
    "        lambda row: row['Equal-weighted'] / (df_ew[df_ew['year'] == row['year']]['deltap'].std() / \n",
    "                                             (len(df_ew[df_ew['year'] == row['year']]) ** 0.5)), axis=1)\n",
    "    \n",
    "    # Calculate t-stat for full sample\n",
    "    ew_full_mean = Illiq_port_ew_full[1]\n",
    "    ew_full_std = df_ew['deltap'].std()\n",
    "    ew_full_size = len(df_ew)\n",
    "    ew_full_t_stat = ew_full_mean / (ew_full_std / (ew_full_size ** 0.5))\n",
    "    \n",
    "    # Issurance weighted\n",
    "    df['issurance'] = df['offering_amt'] * df['principal_amt'] * df['offering_price'] / 100 / 1000000\n",
    "    df['value_weighted_deltap'] = df['deltap'] * df['issurance']\n",
    "    df['value_weighted_deltap_lag'] = df['deltap_lag'] * df['issurance']\n",
    "\n",
    "    # Group by day and calculate the sum of the value-weighted columns and issurance\n",
    "    df_vw = df.groupby('trd_exctn_dt').agg(\n",
    "        total_value_weighted_deltap=pd.NamedAgg(column='value_weighted_deltap', aggfunc='sum'),\n",
    "        total_value_weighted_deltap_lag=pd.NamedAgg(column='value_weighted_deltap_lag', aggfunc='sum'),\n",
    "        total_issurance=pd.NamedAgg(column='issurance', aggfunc='sum')\n",
    "    )\n",
    "\n",
    "    # Calculate the average value-weighted deltap and deltap_lag\n",
    "    df_vw['deltap_vw'] = df_vw['total_value_weighted_deltap'] / df_vw['total_issurance']\n",
    "    df_vw['deltap_lag_vw'] = df_vw['total_value_weighted_deltap_lag'] / df_vw['total_issurance']\n",
    "    df_vw['year'] = df_vw.index.year\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    Illiq_port_vw = df_vw.groupby(['year'])[['deltap_vw','deltap_lag_vw']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    Illiq_port_vw = Illiq_port_vw.reset_index()\n",
    "    Illiq_port_vw.columns = ['year','Issuance-weighted']\n",
    "    Illiq_port_vw = Illiq_port_vw.dropna(subset=['Issuance-weighted'])\n",
    "\n",
    "    # for full equal weighted porfolio illiquidity\n",
    "    df_vw['full'] = 1\n",
    "    Illiq_port_vw_full = df_vw.groupby(['full'] )[['deltap_vw','deltap_lag_vw']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "        \n",
    "    # Calculate t-stat for issuance-weighted illiquidity\n",
    "    Illiq_port_vw['IW t-stat'] = Illiq_port_vw.apply(\n",
    "        lambda row: row['Issuance-weighted'] / (df_vw[df_vw['year'] == row['year']]['deltap_vw'].std() / \n",
    "                                                 (len(df_vw[df_vw['year'] == row['year']]) ** 0.5)), axis=1)\n",
    "\n",
    "    # Calculate t-stat for full sample\n",
    "    iw_full_mean = Illiq_port_vw_full[1]\n",
    "    iw_full_std = df_vw['deltap_vw'].std()\n",
    "    iw_full_size = len(df_vw)\n",
    "    iw_full_t_stat = iw_full_mean / (iw_full_std / (iw_full_size ** 0.5))\n",
    "\n",
    "    table2_port = pd.DataFrame({\n",
    "        'Year': Illiq_port_vw['year'],\n",
    "        'Equal weighted': Illiq_port_ew['Equal-weighted'],\n",
    "        'EW t stat': Illiq_port_ew['EW t-stat'],\n",
    "        'Issuance weighted': Illiq_port_vw['Issuance-weighted'],\n",
    "        'IW t stat': Illiq_port_vw['IW t-stat']\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Equal weighted': Illiq_port_ew_full,\n",
    "        'EW t stat': ew_full_t_stat,\n",
    "        'Issuance weighted': Illiq_port_vw_full,\n",
    "        'IW t stat': iw_full_t_stat\n",
    "    })\n",
    "\n",
    "    table2_port = pd.concat([table2_port, overall_data], ignore_index=True)\n",
    "    \n",
    "    return table2_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_port_paper = calc_illiquidity.calc_annual_illiquidity_table_portfolio(df_paper)\n",
    "table2_port_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_port_new = calc_illiquidity.calc_annual_illiquidity_table_portfolio(df_new)\n",
    "table2_port_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Panel C Implied by Quoted Bid-Ask Spreads: Annual Implied Illiquidity Using Monthly Quoted Bid-Ask Spread\n",
    "\n",
    "In this section, we focus on analyzing the illiquidity implied by quoted bid-ask spreads of corporate bonds on an annual basis using `calc_annual_illiquidity_table_spd`. \n",
    "\n",
    "\n",
    "- For each year, calculates the mean and median of the monthly `t_spread`, which represent the implied gamma. \n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean and median implied illiquidity for each year, as well as for the full sample period.\n",
    "\n",
    "By computing these statistics, the function provides insights into the liquidity of the corporate bond market as implied by the bid-ask spreads over time. As shown in the paper, not only does the quoted bid-ask spread fail to capture the overall level of illiquidity, but it also fails to explain the cross-sectional variation in bond illiquidity and its asset pricing implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annual_illiquidity_table_spd(df):\n",
    "    \"\"\"Calculate mean and median gamma implied by quoted bid-ask spreads by year.\n",
    "    \"\"\"\n",
    "    df_unique = df.groupby(['cusip', 'month_year'])['t_spread'].first().reset_index()\n",
    "    df_unique['year'] = df_unique['month_year'].dt.year  \n",
    "    df_unique = df_unique.sort_values(by='month_year')\n",
    "\n",
    "    Illiq_mean_table = df_unique.groupby('year')['t_spread'].mean()\n",
    "    overall_illiq_mean = df_unique['t_spread'].mean()\n",
    "    overall_illiq_median = df_unique['t_spread'].median()\n",
    "    \n",
    "    table2_spd = pd.DataFrame({\n",
    "        'Year': Illiq_mean_table.index,\n",
    "        'Mean implied gamma': df_unique.groupby('year')['t_spread'].mean(),\n",
    "        'Median implied gamma': df_unique.groupby('year')['t_spread'].median(),\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean implied gamma': [overall_illiq_mean], \n",
    "        'Median implied gamma': [overall_illiq_median]\n",
    "    })\n",
    "    \n",
    "    table2_spd = pd.concat([table2_spd, overall_data], ignore_index=True)\n",
    "    \n",
    "    return table2_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_spd_paper = calc_illiquidity.calc_annual_illiquidity_table_spd(df_paper) \n",
    "table2_spd_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_spd_new = calc_illiquidity.calc_annual_illiquidity_table_spd(df_new) \n",
    "table2_spd_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Monthly Illiquidity Per Bond and Average Illiquidity By Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_illiquidity(illiq_daily_paper, illiq_daily_summary_paper, \"2003-2009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_illiquidity(illiq_daily_new, illiq_daily_summary_new, \"2009-2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_illiquidity(mmn_paper, illiq_daily_summary_mmn_paper, \"MMN_Corrected, 2003-2009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of illiquidity by year\n",
    "plot.plot_illiquidity(mmn_new, illiq_daily_summary_mmn_new, \"MMN_Corrected, 2009-2023\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
