{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illiquidity Calculation\n",
    "\n",
    "  - This notebook walks through illiquidity calculations based on methodology in The Illiquidity of Corporate Bonds, Bao, Pan, and Wang (2010).\n",
    "\n",
    "  - In order to avoid re-running the notebook every time it changes (it changes often, even by the act of opening it) and to only rerun it if meaningful changes have been made, the build system only looks for changes in the plaintext version of the notebook. That is, the notebook is converted to a Python script via [nbconvert](https://nbconvert.readthedocs.io/en/latest/), which is often packaged with Jupyter.\n",
    "  Then, DoIt looks for changes to the Python version. If it detects a difference, then the notebook is re-run. (Note, that you could also convert to a Markdown file with \n",
    "  [JupyText](https://github.com/mwouts/jupytext). However, this package is often not packaged with Jupyter.)\n",
    "  - Since we want to use Jupyter Notebooks for exploratory reports, we want to keep fully-computed versions of the notebook (with the output intact). However, earlier I said that I strip the notebook of its output before committing to version control. Well, to keep the output, every time PyDoit runs the notebook, it outputs an HTML version of the freshly run notebook and saves that HTML report in the `output` directory. That way, you will be able to view the finished report at any time without having to open Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "OUTPUT_DIR = config.OUTPUT_DIR\n",
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc_tools\n",
    "import load_wrds_bondret\n",
    "import load_opensource\n",
    "import data_processing as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Clean Merged Data for Daily Illiquidity Calculation\n",
    "\n",
    "Before calculating illiquidity measures, it's essential to ensure that our corporate bond data is accurate and relevant. The `clean_merged_data` function takes care of preparing the pre-cleaned merged monthly and daily data by performing several critical cleaning steps:\n",
    "\n",
    "- Loads and merges the relevant datasets within the specified date range.\n",
    "- Removes any records with missing crucial price information and sorts the data chronologically.\n",
    "- Adjusts for trade execution dates by incorporating a time lag to identify consecutive trades for the same bond, and filters out those that do not fall within a one-week window, accounting for holidays.\n",
    "- Consolidates the cleaned data, readying it for the subsequent illiquidity analysis.\n",
    "\n",
    "This step is crucial to ensure that the subsequent calculations are based on a dataset that reflects true trading activity without distortions from missing data or trades too far apart in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_merged_data(start_date, end_date):\n",
    "    \"\"\"Load merged, pre-cleaned daily and monthly corporate bond data for a given time interval,\n",
    "    and conduct additional cleaning for illiquidity calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    # load and merge pre-cleaned daily and monthly data\n",
    "    df_daily = load_opensource.load_daily_bond(data_dir=DATA_DIR)\n",
    "    df_bondret = load_wrds_bondret.load_bondret(data_dir=DATA_DIR)\n",
    "    merged_df = data.all_trace_data_merge(df_daily, df_bondret,\n",
    "                                          start_date = start_date, end_date = end_date)\n",
    "    merged_df = data.sample_selection(merged_df, start_date = start_date,\n",
    "                                      end_date = end_date)\n",
    "    \n",
    "    # Clean data\n",
    "    merged_df = merged_df.dropna(subset=['prclean'])\n",
    "    merged_df = merged_df.sort_values(by='trd_exctn_dt')\n",
    "    merged_df['month_year'] = pd.to_datetime(merged_df['trd_exctn_dt']).dt.to_period('M') \n",
    "\n",
    "    # Lags days for day_counts\n",
    "    merged_df['trd_exctn_dt_lag'] = merged_df.\\\n",
    "        groupby('cusip')['trd_exctn_dt'].shift(1)\n",
    "    dfDC = merged_df.dropna(subset=['trd_exctn_dt_lag'])\n",
    "\n",
    "    # Generate a list of U.S. holidays over this period\n",
    "    # Only include \"daily\" return if the gap between trades is less than 1-Week \n",
    "    calendar = USFederalHolidayCalendar()\n",
    "    holidays = calendar.holidays(start_date, end_date)  # 01JUL2002  # 31DEC2022\n",
    "    holiday_date_list = holidays.date.tolist()\n",
    "\n",
    "    dfDC['n']  = np.busday_count(dfDC['trd_exctn_dt_lag'].values.astype('M8[D]') , \n",
    "                                        dfDC['trd_exctn_dt'].values.astype('M8[D]'),\n",
    "                                        holidays = holiday_date_list)\n",
    "\n",
    "    df = merged_df.merge(dfDC[['cusip', 'trd_exctn_dt', 'n']],\n",
    "                         left_on = ['cusip','trd_exctn_dt'], \n",
    "                         right_on = ['cusip','trd_exctn_dt'], how = \"left\")\n",
    "    del(dfDC)\n",
    "    df = df[df.n <= 7]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\4123699928.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfDC['n']  = np.busday_count(dfDC['trd_exctn_dt_lag'].values.astype('M8[D]') ,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>trd_exctn_dt</th>\n",
       "      <th>prclean</th>\n",
       "      <th>prfull</th>\n",
       "      <th>acclast</th>\n",
       "      <th>accpmt</th>\n",
       "      <th>accall</th>\n",
       "      <th>ytm</th>\n",
       "      <th>ytmt</th>\n",
       "      <th>qvolume</th>\n",
       "      <th>...</th>\n",
       "      <th>first_interest_date</th>\n",
       "      <th>last_interest_date</th>\n",
       "      <th>ncoups</th>\n",
       "      <th>amount_outstanding</th>\n",
       "      <th>n_mr</th>\n",
       "      <th>tmt</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>trd_exctn_dt_lag</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>36962GUR3</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>105.974001</td>\n",
       "      <td>109.256640</td>\n",
       "      <td>3.282639</td>\n",
       "      <td>18.185417</td>\n",
       "      <td>21.468056</td>\n",
       "      <td>0.014644</td>\n",
       "      <td>0.014644</td>\n",
       "      <td>2500000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-11-03</td>\n",
       "      <td>2003-11-03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>700000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>370442AR6</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>95.332902</td>\n",
       "      <td>96.237346</td>\n",
       "      <td>0.904444</td>\n",
       "      <td>55.335556</td>\n",
       "      <td>56.240000</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>0.078446</td>\n",
       "      <td>889000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1996-03-01</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.666667</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>92344SAE0</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>106.907199</td>\n",
       "      <td>108.713796</td>\n",
       "      <td>1.806597</td>\n",
       "      <td>2.672569</td>\n",
       "      <td>4.479167</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>0.033530</td>\n",
       "      <td>9000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2002-12-15</td>\n",
       "      <td>2006-06-15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2446000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.680556</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>949746CC3</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>111.335999</td>\n",
       "      <td>112.403360</td>\n",
       "      <td>1.067361</td>\n",
       "      <td>18.125000</td>\n",
       "      <td>19.192361</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2001-02-24</td>\n",
       "      <td>2005-02-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.352778</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>46625HAV2</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>101.786597</td>\n",
       "      <td>102.608819</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>22750000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2003-08-01</td>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.827778</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cusip trd_exctn_dt     prclean      prfull   acclast     accpmt  \\\n",
       "602  36962GUR3   2003-04-15  105.974001  109.256640  3.282639  18.185417   \n",
       "603  370442AR6   2003-04-15   95.332902   96.237346  0.904444  55.335556   \n",
       "604  92344SAE0   2003-04-15  106.907199  108.713796  1.806597   2.672569   \n",
       "605  949746CC3   2003-04-15  111.335999  112.403360  1.067361  18.125000   \n",
       "606  46625HAV2   2003-04-15  101.786597  102.608819  0.822222   0.033333   \n",
       "\n",
       "        accall       ytm      ytmt     qvolume  ...  first_interest_date  \\\n",
       "602  21.468056  0.014644  0.014644   2500000.0  ...           2000-11-03   \n",
       "603  56.240000  0.078446  0.078446    889000.0  ...           1996-03-01   \n",
       "604   4.479167  0.033530  0.033530   9000000.0  ...           2002-12-15   \n",
       "605  19.192361  0.022727  0.022727  50000000.0  ...           2001-02-24   \n",
       "606   0.855556  0.035897  0.035897  22750000.0  ...           2003-08-01   \n",
       "\n",
       "     last_interest_date  ncoups  amount_outstanding  n_mr        tmt  year  \\\n",
       "602          2003-11-03     2.0            700000.0   1.0   1.025000  2003   \n",
       "603          2025-03-01     2.0            500000.0   7.0  22.666667  2003   \n",
       "604          2006-06-15     2.0           2446000.0   7.0   3.680556  2003   \n",
       "605          2005-02-24     2.0           1000000.0   3.0   2.352778  2003   \n",
       "606          2007-08-01     2.0           1000000.0   5.0   4.827778  2003   \n",
       "\n",
       "    month_year  trd_exctn_dt_lag    n  \n",
       "602    2003-04        2003-04-14  1.0  \n",
       "603    2003-04        2003-04-14  1.0  \n",
       "604    2003-04        2003-04-14  1.0  \n",
       "605    2003-04        2003-04-14  1.0  \n",
       "606    2003-04        2003-04-14  1.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = clean_merged_data('2003-04-14', '2009-06-30')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Price Changes and Perform Additional Cleaning\n",
    "\n",
    "In this part of the analysis pipeline, we use the `calc_deltaprc` function to compute daily price changes for corporate bonds, designed to operate on cleaned and merged daily corporate bond trade data.\n",
    "\n",
    "This calculation is based on the Measure of Illiquidity on page 10 and 11 of the peper: $ \\gamma = -\\text{Cov}(p_t - p_{t-1}, p_{t+1} - p_t) $. The process involves several steps:\n",
    "- Calculation of Log Prices: Transform cleaned prices to log prices for more stable numerical properties.\n",
    "- Lagged and Lead Price Changes: Determine the price changes by computing lagged and lead log prices.\n",
    "- Restricting Returns: Ensure that calculated price changes (returns) are within the range of -100% to 100%.\n",
    "- Conversion to Percentage: Change the representation of price changes from decimal to percentage for clarity.\n",
    "- Cleaning Data: Remove entries with incomplete information to maintain the quality of the dataset.\n",
    "- Filtering by Trade Count: Exclude bonds with fewer than 10 trade observations to focus on more reliable data.\n",
    "\n",
    "This function is essential for preparing the bond price data for accurate calculation of financial metrics such as illiquidity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deltaprc(df):\n",
    "    \"\"\"Calculate delta price and delta price_lag for each daily trades with additional cleaning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate lagged and lead log prices, and corresponding delta p (percentage returns)\n",
    "    df['logprc']     = np.log(df['prclean'])\n",
    "    df['logprc_lag'] = df.groupby( 'cusip' )['logprc'].shift(1)\n",
    "    df['deltap']     = df ['logprc'] - df ['logprc_lag']\n",
    "\n",
    "    # Restrict log returns to be in the interval [1,1]\n",
    "    df['deltap'] = np.where(df['deltap'] > 1, 1, df['deltap'])\n",
    "    df['deltap'] = np.where(df['deltap'] <-1, -1, df['deltap'])\n",
    "\n",
    "    # Convert deltap to % i.e. returns in % as opposed to decimals\n",
    "    df['deltap'] = df['deltap'] * 100\n",
    "    \n",
    "    df['logprc_lead'] = df.groupby( 'cusip' )['logprc'].shift(-1)\n",
    "    df['deltap_lag'] = df ['logprc_lead'] - df ['logprc']\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] > 1, 1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] <-1, -1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = df['deltap_lag'] * 100\n",
    "\n",
    "    # df.isna().sum()\n",
    "    # df_bondret.columns\n",
    "    \n",
    "    # Drop NAs in deltap, deltap_lag and bonds < 10 observations of the paired price changes\n",
    "    df_final = df.dropna(subset=['deltap', 'deltap_lag', 'prclean'])\n",
    "    df_final['trade_counts'] = df_final.groupby(['cusip', 'year'])['deltap'].transform(\"count\")\n",
    "    df_final = df_final[df_final['trade_counts'] >= 10]    \n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1364041051.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['trade_counts'] = df_final.groupby(['cusip', 'year'])['deltap'].transform(\"count\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>trd_exctn_dt</th>\n",
       "      <th>prclean</th>\n",
       "      <th>prfull</th>\n",
       "      <th>acclast</th>\n",
       "      <th>accpmt</th>\n",
       "      <th>accall</th>\n",
       "      <th>ytm</th>\n",
       "      <th>ytmt</th>\n",
       "      <th>qvolume</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "      <th>trd_exctn_dt_lag</th>\n",
       "      <th>n</th>\n",
       "      <th>logprc</th>\n",
       "      <th>logprc_lag</th>\n",
       "      <th>deltap</th>\n",
       "      <th>logprc_lead</th>\n",
       "      <th>deltap_lag</th>\n",
       "      <th>trade_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>345397SQ7</td>\n",
       "      <td>2003-04-16</td>\n",
       "      <td>103.468001</td>\n",
       "      <td>104.176334</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>22.520833</td>\n",
       "      <td>23.229167</td>\n",
       "      <td>0.055476</td>\n",
       "      <td>0.055476</td>\n",
       "      <td>10334000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.639262</td>\n",
       "      <td>4.633919</td>\n",
       "      <td>0.534345</td>\n",
       "      <td>4.639275</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>370425RJ1</td>\n",
       "      <td>2003-04-16</td>\n",
       "      <td>104.210101</td>\n",
       "      <td>106.569545</td>\n",
       "      <td>2.359444</td>\n",
       "      <td>23.975000</td>\n",
       "      <td>26.334444</td>\n",
       "      <td>0.031076</td>\n",
       "      <td>0.031076</td>\n",
       "      <td>1003000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.646409</td>\n",
       "      <td>4.646475</td>\n",
       "      <td>-0.006620</td>\n",
       "      <td>4.647230</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>260543BU6</td>\n",
       "      <td>2003-04-16</td>\n",
       "      <td>103.132500</td>\n",
       "      <td>105.201945</td>\n",
       "      <td>2.069444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.069444</td>\n",
       "      <td>0.042376</td>\n",
       "      <td>0.042376</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.636015</td>\n",
       "      <td>4.627253</td>\n",
       "      <td>0.876200</td>\n",
       "      <td>4.629933</td>\n",
       "      <td>-0.608155</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>191219BF0</td>\n",
       "      <td>2003-04-16</td>\n",
       "      <td>111.127899</td>\n",
       "      <td>113.843177</td>\n",
       "      <td>2.715278</td>\n",
       "      <td>22.984028</td>\n",
       "      <td>25.699306</td>\n",
       "      <td>0.035189</td>\n",
       "      <td>0.035189</td>\n",
       "      <td>1130000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.710682</td>\n",
       "      <td>4.692768</td>\n",
       "      <td>1.791380</td>\n",
       "      <td>4.695206</td>\n",
       "      <td>-1.547620</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>37042GD52</td>\n",
       "      <td>2003-04-16</td>\n",
       "      <td>96.987273</td>\n",
       "      <td>97.108107</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>4.450694</td>\n",
       "      <td>4.571528</td>\n",
       "      <td>0.075946</td>\n",
       "      <td>0.074771</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003-04</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.574580</td>\n",
       "      <td>4.585009</td>\n",
       "      <td>-1.042888</td>\n",
       "      <td>4.574582</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cusip trd_exctn_dt     prclean      prfull   acclast     accpmt  \\\n",
       "1225  345397SQ7   2003-04-16  103.468001  104.176334  0.708333  22.520833   \n",
       "1226  370425RJ1   2003-04-16  104.210101  106.569545  2.359444  23.975000   \n",
       "1227  260543BU6   2003-04-16  103.132500  105.201945  2.069444   0.000000   \n",
       "1228  191219BF0   2003-04-16  111.127899  113.843177  2.715278  22.984028   \n",
       "1229  37042GD52   2003-04-16   96.987273   97.108107  0.120833   4.450694   \n",
       "\n",
       "         accall       ytm      ytmt     qvolume  ...  year  month_year  \\\n",
       "1225  23.229167  0.055476  0.055476  10334000.0  ...  2003     2003-04   \n",
       "1226  26.334444  0.031076  0.031076   1003000.0  ...  2003     2003-04   \n",
       "1227   2.069444  0.042376  0.042376   1035000.0  ...  2003     2003-04   \n",
       "1228  25.699306  0.035189  0.035189   1130000.0  ...  2003     2003-04   \n",
       "1229   4.571528  0.075946  0.074771     25000.0  ...  2003     2003-04   \n",
       "\n",
       "      trd_exctn_dt_lag    n    logprc  logprc_lag    deltap logprc_lead  \\\n",
       "1225        2003-04-15  1.0  4.639262    4.633919  0.534345    4.639275   \n",
       "1226        2003-04-15  1.0  4.646409    4.646475 -0.006620    4.647230   \n",
       "1227        2003-04-15  1.0  4.636015    4.627253  0.876200    4.629933   \n",
       "1228        2003-04-15  1.0  4.710682    4.692768  1.791380    4.695206   \n",
       "1229        2003-04-15  1.0  4.574580    4.585009 -1.042888    4.574582   \n",
       "\n",
       "      deltap_lag trade_counts  \n",
       "1225    0.001255          180  \n",
       "1226    0.082107          178  \n",
       "1227   -0.608155          169  \n",
       "1228   -1.547620          172  \n",
       "1229    0.000207          153  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = calc_deltaprc(cleaned_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Annual Illiquidity Metrics Calculation\n",
    "\n",
    "This step involves using the `calc_annual_illiquidity_table_daily` function to calculate and summarize annual illiquidity metrics for corporate bonds. The function takes daily bond data as input and computes several statistics that capture the illiquidity of bonds on an annual basis.\n",
    "\n",
    "- Computes the illiquidity for each bond and month by taking the negative of the covariance between daily price changes (`deltap`) and their lagged values (`deltap_lag`).\n",
    "\n",
    "- Aggregated the monthly illiquidity measures to obtain annual statistics, including mean and median illiquidity.\n",
    "\n",
    "- Calculates t-statistics for the mean illiquidity of each bond and year and determines the percentage of these t-stats that are significant (>= 1.96).\n",
    "\n",
    "- Calculates robust t-stats are calculated using OLS with HAC (heteroskedasticity and autocorrelation consistent) standard errors.\n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean and median illiquidity, the percentage of significant t-statistics, and robust t-statistics for each year, as well as for the full sample period.\n",
    "\n",
    "This comprehensive illiquidity metric calculation allows us to understand the annual and overall liquidity characteristics of the corporate bond market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_annual_illiquidity_table_daily(df):\n",
    "    \"\"\"Calculate illiquidity = -cov(deltap, deltap_lag) using daily data, groupby in month,\n",
    "    present as annual mean, median, percetage t >= 1.96, and robust t-stat. \n",
    "    \"\"\"\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    Illiq_month = df.groupby(['cusip','month_year'] )[['deltap','deltap_lag']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    Illiq_month = Illiq_month.reset_index()\n",
    "    Illiq_month.columns = ['cusip','month_year','illiq']\n",
    "    Illiq_month['year'] = Illiq_month['month_year'].dt.year\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq'])\n",
    "    # Illiq_month = Illiq_month[Illiq_month['illiq'] < 2000]\n",
    "\n",
    "    overall_illiq_mean = np.mean(Illiq_month['illiq'])\n",
    "    overall_illiq_median = Illiq_month['illiq'].median()\n",
    "\n",
    "    # Calculate t-statistics for each cusip in each year\n",
    "    Illiq_month['t_stat'] = Illiq_month.groupby(['cusip', 'year'])['illiq'].transform(\n",
    "        lambda x: (x.mean() / x.sem()) if x.sem() > 0 else np.nan)\n",
    "\n",
    "    # Identify the entries with t-stat >= 1.96 and calculate the percentage of significant t-stats for each year\n",
    "    Illiq_month['significant'] = Illiq_month['t_stat'] >= 1.96\n",
    "    percent_significant = Illiq_month.groupby('year')['significant'].mean() * 100\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq', 't_stat'])\n",
    "    overall_percent_significant = Illiq_month['significant'].mean() * 100\n",
    "    \n",
    "    # Calculate robust t-stat for each year\n",
    "    def get_robust_t_stat(group):\n",
    "        \"\"\"Run OLS on a constant term only (mean of illiq) to get the intercept's t-stat.\"\"\"\n",
    "        X = add_constant(group['illiq'])\n",
    "        ols_result = OLS(group['illiq'], X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "\n",
    "    robust_t_stats = Illiq_month.groupby('year').apply(get_robust_t_stat)\n",
    "    \n",
    "    \n",
    "    def calculate_overall_robust_t_stat(series):\n",
    "        X = add_constant(series)\n",
    "        ols_result = OLS(series, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "    # Call the function and assign the result to overall_robust_t_stat\n",
    "    overall_robust_t_stat = calculate_overall_robust_t_stat(Illiq_month['illiq'].dropna())\n",
    "\n",
    "    # Combine the results\n",
    "    table2_daily = pd.DataFrame({\n",
    "        'Year': robust_t_stats.index,\n",
    "        'Mean_illiq': Illiq_month.groupby('year')['illiq'].mean(),\n",
    "        'Median_illiq': Illiq_month.groupby('year')['illiq'].median(),\n",
    "        'Per_t_greater_1_96': percent_significant,\n",
    "        'Robust_t_stat': robust_t_stats.values\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean_illiq': [overall_illiq_mean],\n",
    "        'Median_illiq': [overall_illiq_median],\n",
    "        'Per_t_greater_1_96': [overall_percent_significant],\n",
    "        'Robust_t_stat': [overall_robust_t_stat]\n",
    "    })\n",
    "\n",
    "    table2_daily = pd.concat([table2_daily, overall_data], ignore_index=True)\n",
    "\n",
    "    return table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2073/47561 [00:00<00:09, 4567.12it/s]C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:11184: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  base_cov = np.cov(mat.T, ddof=ddof)\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "C:\\Users\\zhang\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "100%|██████████| 47561/47561 [00:07<00:00, 6473.89it/s]\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:34: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  robust_t_stats = Illiq_month.groupby('year').apply(get_robust_t_stat)\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_17748\\1032397038.py:43: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  return abs(ols_result.tvalues[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mean_illiq</th>\n",
       "      <th>Median_illiq</th>\n",
       "      <th>Per_t_greater_1_96</th>\n",
       "      <th>Robust_t_stat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>1.012431</td>\n",
       "      <td>0.118649</td>\n",
       "      <td>77.283913</td>\n",
       "      <td>2.541571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>1.054907</td>\n",
       "      <td>0.061028</td>\n",
       "      <td>77.409016</td>\n",
       "      <td>10.183842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.851126</td>\n",
       "      <td>0.040949</td>\n",
       "      <td>80.365932</td>\n",
       "      <td>4.067339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>0.409025</td>\n",
       "      <td>0.036393</td>\n",
       "      <td>88.037910</td>\n",
       "      <td>6.715445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>1.116023</td>\n",
       "      <td>0.064962</td>\n",
       "      <td>87.447828</td>\n",
       "      <td>1.815385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008</td>\n",
       "      <td>13.271568</td>\n",
       "      <td>0.232770</td>\n",
       "      <td>67.607452</td>\n",
       "      <td>20.787846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009</td>\n",
       "      <td>17.980450</td>\n",
       "      <td>0.334487</td>\n",
       "      <td>69.398431</td>\n",
       "      <td>1.186281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Full</td>\n",
       "      <td>3.120078</td>\n",
       "      <td>0.072691</td>\n",
       "      <td>79.679268</td>\n",
       "      <td>17.057181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Mean_illiq  Median_illiq  Per_t_greater_1_96  Robust_t_stat\n",
       "0  2003    1.012431      0.118649           77.283913       2.541571\n",
       "1  2004    1.054907      0.061028           77.409016      10.183842\n",
       "2  2005    0.851126      0.040949           80.365932       4.067339\n",
       "3  2006    0.409025      0.036393           88.037910       6.715445\n",
       "4  2007    1.116023      0.064962           87.447828       1.815385\n",
       "5  2008   13.271568      0.232770           67.607452      20.787846\n",
       "6  2009   17.980450      0.334487           69.398431       1.186281\n",
       "7  Full    3.120078      0.072691           79.679268      17.057181"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2_daily = calc_annual_illiquidity_table_daily(df)\n",
    "table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l|rrrr}\n",
      "\\toprule\n",
      "Year & Mean_illiq & Median_illiq & Per_t_greater_1_96 & Robust_t_stat \\\\\n",
      "\\midrule\n",
      "2003 & 1.01 & 0.12 & 77.28 & 2.54 \\\\\n",
      "2004 & 1.05 & 0.06 & 77.41 & 10.18 \\\\\n",
      "2005 & 0.85 & 0.04 & 80.37 & 4.07 \\\\\n",
      "2006 & 0.41 & 0.04 & 88.04 & 6.72 \\\\\n",
      "2007 & 1.12 & 0.06 & 87.45 & 1.82 \\\\\n",
      "2008 & 13.27 & 0.23 & 67.61 & 20.79 \\\\\n",
      "2009 & 17.98 & 0.33 & 69.40 & 1.19 \\\\\n",
      "Full & 3.12 & 0.07 & 79.68 & 17.06 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_table2_daily = table2_daily.to_latex(index=False, float_format=\"{:0.2f}\".format, column_format='l|rrrr', escape=False)\n",
    "print(latex_table2_daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annual Implied Gamma Calculation Using Quoted Bid-Ask Spread\n",
    "\n",
    "In this section, we focus on analyzing the illiquidity implied by quoted bid-ask spreads of corporate bonds on an annual basis using `calc_annual_illiquidity_table_spd`. \n",
    "\n",
    "\n",
    "- For each year, calculates the mean and median of the monthly 't_spread', which represent the implied gamma. \n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean and median implied illiquidity for each year, as well as for the full sample period.\n",
    "\n",
    "By computing these statistics, the function provides insights into the liquidity of the corporate bond market as implied by the bid-ask spreads over time. As shown in the paper, not only does the quoted bid-ask spread fail to capture the overall level of illiquidity, but it also fails to explain the cross-sectional variation in bond illiquidity and its asset pricing implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annual_illiquidity_table_spd(df):\n",
    "    \"\"\"Calculate mean and median gamma implied by quoted bid-ask spreads by year.\n",
    "    \"\"\"\n",
    "    df_unique = df.groupby(['cusip', 'month_year'])['t_spread'].first().reset_index()\n",
    "    df_unique['year'] = df_unique['month_year'].dt.year  \n",
    "    df_unique = df_unique.sort_values(by='month_year')\n",
    "\n",
    "    Illiq_mean_table = df_unique.groupby('year')['t_spread'].mean()\n",
    "    overall_illiq_mean = df_unique['t_spread'].mean()\n",
    "    overall_illiq_median = df_unique['t_spread'].median()\n",
    "    \n",
    "    table2_spd = pd.DataFrame({\n",
    "        'Year': Illiq_mean_table.index,\n",
    "        'Mean implied gamma': df_unique.groupby('year')['t_spread'].mean(),\n",
    "        'Median implied gamma': df_unique.groupby('year')['t_spread'].median(),\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean implied gamma': [overall_illiq_mean],\n",
    "        'Median implied gamma': [overall_illiq_median]\n",
    "    })\n",
    "    \n",
    "    table2_spd = pd.concat([table2_spd, overall_data], ignore_index=True)\n",
    "    \n",
    "    return table2_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mean implied gamma</th>\n",
       "      <th>Median implied gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>0.004536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0.003306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>0.003143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.004374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008</td>\n",
       "      <td>0.012438</td>\n",
       "      <td>0.008984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2009</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.012348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Full</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>0.004244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Mean implied gamma  Median implied gamma\n",
       "0  2003            0.006595              0.004536\n",
       "1  2004            0.005372              0.003603\n",
       "2  2005            0.004687              0.003306\n",
       "3  2006            0.004279              0.003143\n",
       "4  2007            0.005710              0.004374\n",
       "5  2008            0.012438              0.008984\n",
       "6  2009            0.015974              0.012348\n",
       "7  Full            0.006566              0.004244"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2_spd = calc_annual_illiquidity_table_spd(df) \n",
    "# by multiplying these values by 5, we get approximately the same result as the one in the paper\n",
    "table2_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l|rr}\n",
      "\\toprule\n",
      "Year & Mean implied gamma & Median implied gamma \\\\\n",
      "\\midrule\n",
      "2003 & 0.01 & 0.00 \\\\\n",
      "2004 & 0.01 & 0.00 \\\\\n",
      "2005 & 0.00 & 0.00 \\\\\n",
      "2006 & 0.00 & 0.00 \\\\\n",
      "2007 & 0.01 & 0.00 \\\\\n",
      "2008 & 0.01 & 0.01 \\\\\n",
      "2009 & 0.02 & 0.01 \\\\\n",
      "Full & 0.01 & 0.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_table2_spd = table2_final_spd.to_latex(index=False, float_format=\"{:0.2f}\".format, column_format='l|rr', escape=False)\n",
    "print(latex_table2_spd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
