{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illiquidity Calculation\n",
    "\n",
    "  - This notebook walks through illiquidity calculations based on methodology in The Illiquidity of Corporate Bonds, Bao, Pan, and Wang (2010).\n",
    "\n",
    "  - In order to avoid re-running the notebook every time it changes (it changes often, even by the act of opening it) and to only rerun it if meaningful changes have been made, the build system only looks for changes in the plaintext version of the notebook. That is, the notebook is converted to a Python script via [nbconvert](https://nbconvert.readthedocs.io/en/latest/), which is often packaged with Jupyter.\n",
    "  Then, DoIt looks for changes to the Python version. If it detects a difference, then the notebook is re-run. (Note, that you could also convert to a Markdown file with \n",
    "  [JupyText](https://github.com/mwouts/jupytext). However, this package is often not packaged with Jupyter.)\n",
    "  - Since we want to use Jupyter Notebooks for exploratory reports, we want to keep fully-computed versions of the notebook (with the output intact). However, earlier I said that I strip the notebook of its output before committing to version control. Well, to keep the output, every time PyDoit runs the notebook, it outputs an HTML version of the freshly run notebook and saves that HTML report in the `output` directory. That way, you will be able to view the finished report at any time without having to open Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "OUTPUT_DIR = config.OUTPUT_DIR\n",
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc_tools\n",
    "import load_wrds_bondret\n",
    "import load_opensource\n",
    "import data_processing as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Clean Merged Data for Daily Illiquidity Calculation\n",
    "\n",
    "Before calculating illiquidity measures, it's essential to ensure that our corporate bond data is accurate and relevant. The `clean_merged_data` function takes care of preparing the pre-cleaned merged monthly and daily data by performing several critical cleaning steps:\n",
    "\n",
    "- Loads and merges the relevant datasets within the specified date range.\n",
    "- Removes any records with missing crucial price information and sorts the data chronologically.\n",
    "- Adjusts for trade execution dates by incorporating a time lag to identify consecutive trades for the same bond, and filters out those that do not fall within a one-week window, accounting for holidays.\n",
    "- Consolidates the cleaned data, readying it for the subsequent illiquidity analysis.\n",
    "\n",
    "This step is crucial to ensure that the subsequent calculations are based on a dataset that reflects true trading activity without distortions from missing data or trades too far apart in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_merged_data(start_date, end_date):\n",
    "    \"\"\"Load merged, pre-cleaned daily and monthly corporate bond data for a given time interval,\n",
    "    and conduct additional cleaning for illiquidity calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    # load and merge pre-cleaned daily and monthly data\n",
    "    df_daily = load_opensource.load_daily_bond(data_dir=DATA_DIR)\n",
    "    df_bondret = load_wrds_bondret.load_bondret(data_dir=DATA_DIR)\n",
    "    merged_df = data.all_trace_data_merge(df_daily, df_bondret,\n",
    "                                          start_date = start_date, end_date = end_date)\n",
    "    merged_df = data.sample_selection(merged_df, start_date = start_date,\n",
    "                                      end_date = end_date)\n",
    "    \n",
    "    # Clean data\n",
    "    merged_df = merged_df.dropna(subset=['prclean'])\n",
    "    merged_df = merged_df.sort_values(by='trd_exctn_dt')\n",
    "    merged_df['month_year'] = pd.to_datetime(merged_df['trd_exctn_dt']).dt.to_period('M') \n",
    "\n",
    "    # Lags days for day_counts\n",
    "    merged_df['trd_exctn_dt_lag'] = merged_df.\\\n",
    "        groupby('cusip')['trd_exctn_dt'].shift(1)\n",
    "    dfDC = merged_df.dropna(subset=['trd_exctn_dt_lag'])\n",
    "\n",
    "    # Generate a list of U.S. holidays over this period\n",
    "    # Only include \"daily\" return if the gap between trades is less than 1-Week \n",
    "    calendar = USFederalHolidayCalendar()\n",
    "    holidays = calendar.holidays(start_date, end_date)  # 01JUL2002  # 31DEC2022\n",
    "    holiday_date_list = holidays.date.tolist()\n",
    "\n",
    "    dfDC['n']  = np.busday_count(dfDC['trd_exctn_dt_lag'].values.astype('M8[D]') , \n",
    "                                        dfDC['trd_exctn_dt'].values.astype('M8[D]'),\n",
    "                                        holidays = holiday_date_list)\n",
    "\n",
    "    df = merged_df.merge(dfDC[['cusip', 'trd_exctn_dt', 'n']],\n",
    "                         left_on = ['cusip','trd_exctn_dt'], \n",
    "                         right_on = ['cusip','trd_exctn_dt'], how = \"left\")\n",
    "    del(dfDC)\n",
    "    df = df[df.n <= 7]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cleaned_df \u001b[38;5;241m=\u001b[39m \u001b[43mclean_merged_data\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2003-04-14\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2009-06-30\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m cleaned_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_merged_data('2003-04-14', '2009-06-30')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Calculate Price Changes and Perform Additional Cleaning\n",
    "\n",
    "In this part of the analysis pipeline, we use the `calc_deltaprc` function to compute daily price changes for corporate bonds, designed to operate on cleaned and merged daily corporate bond trade data.\n",
    "\n",
    "This calculation is based on the Measure of Illiquidity on page 10 and 11 of the peper: $ \\gamma = -\\text{Cov}(p_t - p_{t-1}, p_{t+1} - p_t) $. The process involves several steps:\n",
    "- Calculation of Log Prices: Transform cleaned prices to log prices for more stable numerical properties.\n",
    "- Lagged and Lead Price Changes: Determine the price changes by computing lagged and lead log prices.\n",
    "- Restricting Returns: Ensure that calculated price changes (returns) are within the range of -100% to 100%.\n",
    "- Conversion to Percentage: Change the representation of price changes from decimal to percentage for clarity.\n",
    "- Cleaning Data: Remove entries with incomplete information to maintain the quality of the dataset.\n",
    "- Filtering by Trade Count: Exclude bonds with fewer than 10 trade observations to focus on more reliable data.\n",
    "\n",
    "This function is essential for preparing the bond price data for accurate calculation of financial metrics such as illiquidity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deltaprc(df):\n",
    "    \"\"\"Calculate delta price and delta price_lag for each daily trades with additional cleaning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate lagged and lead log prices, and corresponding delta p (percentage returns)\n",
    "    df['logprc']     = np.log(df['prclean'])\n",
    "    df['logprc_lag'] = df.groupby( 'cusip' )['logprc'].shift(1)\n",
    "    df['deltap']     = df ['logprc'] - df ['logprc_lag']\n",
    "\n",
    "    # Restrict log returns to be in the interval [1,1]\n",
    "    df['deltap'] = np.where(df['deltap'] > 1, 1, df['deltap'])\n",
    "    df['deltap'] = np.where(df['deltap'] <-1, -1, df['deltap'])\n",
    "\n",
    "    # Convert deltap to % i.e. returns in % as opposed to decimals\n",
    "    df['deltap'] = df['deltap'] * 100\n",
    "    \n",
    "    df['logprc_lead'] = df.groupby( 'cusip' )['logprc'].shift(-1)\n",
    "    df['deltap_lag'] = df ['logprc_lead'] - df ['logprc']\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] > 1, 1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] <-1, -1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = df['deltap_lag'] * 100\n",
    "\n",
    "    # df.isna().sum()\n",
    "    # df_bondret.columns\n",
    "    \n",
    "    # Drop NAs in deltap, deltap_lag and bonds < 10 observations of the paired price changes\n",
    "    df_final = df.dropna(subset=['deltap', 'deltap_lag', 'prclean'])\n",
    "    df_final['trade_counts'] = df_final.groupby(['cusip', 'year'])['deltap'].transform(\"count\")\n",
    "    df_final = df_final[df_final['trade_counts'] >= 10]    \n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calc_deltaprc(cleaned_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Annual Illiquidity Metrics Calculation\n",
    "\n",
    "This step involves using the `calc_annual_illiquidity_table_daily` function to calculate and summarize annual illiquidity metrics for corporate bonds. The function takes daily bond data as input and computes several statistics that capture the illiquidity of bonds on an annual basis.\n",
    "\n",
    "- Computes the illiquidity for each bond and month by taking the negative of the covariance between daily price changes (`deltap`) and their lagged values (`deltap_lag`).\n",
    "\n",
    "- Aggregated the monthly illiquidity measures to obtain annual statistics, including mean and median illiquidity.\n",
    "\n",
    "- Calculates t-statistics for the mean illiquidity of each bond and year and determines the percentage of these t-stats that are significant (>= 1.96).\n",
    "\n",
    "- Calculates robust t-stats are calculated using OLS with HAC (heteroskedasticity and autocorrelation consistent) standard errors.\n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean and median illiquidity, the percentage of significant t-statistics, and robust t-statistics for each year, as well as for the full sample period.\n",
    "\n",
    "This comprehensive illiquidity metric calculation allows us to understand the annual and overall liquidity characteristics of the corporate bond market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_annual_illiquidity_table_daily(df):\n",
    "    \"\"\"Calculate illiquidity = -cov(deltap, deltap_lag) using daily data, groupby in month,\n",
    "    present as annual mean, median, percetage t >= 1.96, and robust t-stat. \n",
    "    \"\"\"\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    Illiq_month = df.groupby(['cusip','month_year'] )[['deltap','deltap_lag']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    Illiq_month = Illiq_month.reset_index()\n",
    "    Illiq_month.columns = ['cusip','month_year','illiq']\n",
    "    Illiq_month['year'] = Illiq_month['month_year'].dt.year\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq'])\n",
    "    # Illiq_month = Illiq_month[Illiq_month['illiq'] < 2000]\n",
    "\n",
    "    overall_illiq_mean = np.mean(Illiq_month['illiq'])\n",
    "    overall_illiq_median = Illiq_month['illiq'].median()\n",
    "\n",
    "    # Calculate t-statistics for each cusip in each year\n",
    "    Illiq_month['t_stat'] = Illiq_month.groupby(['cusip', 'year'])['illiq'].transform(\n",
    "        lambda x: (x.mean() / x.sem()) if x.sem() > 0 else np.nan)\n",
    "\n",
    "    # Identify the entries with t-stat >= 1.96 and calculate the percentage of significant t-stats for each year\n",
    "    Illiq_month['significant'] = Illiq_month['t_stat'] >= 1.96\n",
    "    percent_significant = Illiq_month.groupby('year')['significant'].mean() * 100\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq', 't_stat'])\n",
    "    overall_percent_significant = Illiq_month['significant'].mean() * 100\n",
    "    \n",
    "    # Calculate robust t-stat for each year\n",
    "    def get_robust_t_stat(group):\n",
    "        \"\"\"Run OLS on a constant term only (mean of illiq) to get the intercept's t-stat.\"\"\"\n",
    "        X = add_constant(group['illiq'])\n",
    "        ols_result = OLS(group['illiq'], X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "\n",
    "    robust_t_stats = Illiq_month.groupby('year').apply(get_robust_t_stat)\n",
    "    \n",
    "    \n",
    "    def calculate_overall_robust_t_stat(series):\n",
    "        X = add_constant(series)\n",
    "        ols_result = OLS(series, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "    # Call the function and assign the result to overall_robust_t_stat\n",
    "    overall_robust_t_stat = calculate_overall_robust_t_stat(Illiq_month['illiq'].dropna())\n",
    "\n",
    "    # Combine the results\n",
    "    table2_daily = pd.DataFrame({\n",
    "        'Year': robust_t_stats.index,\n",
    "        'Mean_illiq': Illiq_month.groupby('year')['illiq'].mean(),\n",
    "        'Median_illiq': Illiq_month.groupby('year')['illiq'].median(),\n",
    "        'Per_t_greater_1_96': percent_significant,\n",
    "        'Robust_t_stat': robust_t_stats.values\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean_illiq': [overall_illiq_mean],\n",
    "        'Median_illiq': [overall_illiq_median],\n",
    "        'Per_t_greater_1_96': [overall_percent_significant],\n",
    "        'Robust_t_stat': [overall_robust_t_stat]\n",
    "    })\n",
    "\n",
    "    table2_daily = pd.concat([table2_daily, overall_data], ignore_index=True)\n",
    "\n",
    "    return table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_daily = calc_annual_illiquidity_table_daily(df)\n",
    "table2_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annual Implied Gamma Calculation Using Quoted Bid-Ask Spread\n",
    "\n",
    "In this section, we focus on analyzing the illiquidity implied by quoted bid-ask spreads of corporate bonds on an annual basis using `calc_annual_illiquidity_table_spd`. \n",
    "\n",
    "\n",
    "- For each year, calculates the mean and median of the monthly 't_spread', which represent the implied gamma. \n",
    "\n",
    "- Calculate overall statistics across the full sample period.\n",
    "\n",
    "- Compiles all these metrics into a table that presents the mean and median implied illiquidity for each year, as well as for the full sample period.\n",
    "\n",
    "By computing these statistics, the function provides insights into the liquidity of the corporate bond market as implied by the bid-ask spreads over time. As shown in the paper, not only does the quoted bid-ask spread fail to capture the overall level of illiquidity, but it also fails to explain the cross-sectional variation in bond illiquidity and its asset pricing implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annual_illiquidity_table_spd(df):\n",
    "    \"\"\"Calculate mean and median gamma implied by quoted bid-ask spreads by year.\n",
    "    \"\"\"\n",
    "    df_unique = df.groupby(['cusip', 'month_year'])['t_spread'].first().reset_index()\n",
    "    df_unique['year'] = df_unique['month_year'].dt.year  \n",
    "    df_unique = df_unique.sort_values(by='month_year')\n",
    "\n",
    "    Illiq_mean_table = df_unique.groupby('year')['t_spread'].mean()\n",
    "    overall_illiq_mean = df_unique['t_spread'].mean()\n",
    "    overall_illiq_median = df_unique['t_spread'].median()\n",
    "    \n",
    "    table2_spd = pd.DataFrame({\n",
    "        'Year': Illiq_mean_table.index,\n",
    "        'Mean implied gamma': df_unique.groupby('year')['t_spread'].mean(),\n",
    "        'Median implied gamma': df_unique.groupby('year')['t_spread'].median(),\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean implied gamma': [overall_illiq_mean],\n",
    "        'Median implied gamma': [overall_illiq_median]\n",
    "    })\n",
    "    \n",
    "    table2_spd = pd.concat([table2_spd, overall_data], ignore_index=True)\n",
    "    \n",
    "    return table2_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_final_spd = calc_annual_illiquidity_table_spd(df) \n",
    "# by multiplying these values by 5, we get approximately the same result as the one in the paper\n",
    "table2_final_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
