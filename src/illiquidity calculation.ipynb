{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illiquidity Calculation\n",
    "\n",
    "  - This notebook walks through illiquidity calculations based on methodology in The Illiquidity of Corporate Bonds, Bao, Pan, and Wang (2010).\n",
    "\n",
    "  - In order to avoid re-running the notebook every time it changes (it changes often, even by the act of opening it) and to only rerun it if meaningful changes have been made, the build system only looks for changes in the plaintext version of the notebook. That is, the notebook is converted to a Python script via [nbconvert](https://nbconvert.readthedocs.io/en/latest/), which is often packaged with Jupyter.\n",
    "  Then, DoIt looks for changes to the Python version. If it detects a difference, then the notebook is re-run. (Note, that you could also convert to a Markdown file with \n",
    "  [JupyText](https://github.com/mwouts/jupytext). However, this package is often not packaged with Jupyter.)\n",
    "  - Since we want to use Jupyter Notebooks for exploratory reports, we want to keep fully-computed versions of the notebook (with the output intact). However, earlier I said that I strip the notebook of its output before committing to version control. Well, to keep the output, every time PyDoit runs the notebook, it outputs an HTML version of the freshly run notebook and saves that HTML report in the `output` directory. That way, you will be able to view the finished report at any time without having to open Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "OUTPUT_DIR = config.OUTPUT_DIR\n",
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc_tools\n",
    "import load_wrds_bondret\n",
    "import load_opensource\n",
    "import data_processing as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_merged_data(start_date, end_date):\n",
    "    \"\"\"Load merged, pre-cleaned daily and monthly corporate bond data for a given time interval,\n",
    "    and conduct additional cleaning for illiquidity calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    # load and merge pre-cleaned daily and monthly data\n",
    "    df_daily = load_opensource.load_daily_bond(data_dir=DATA_DIR)\n",
    "    df_bondret = load_wrds_bondret.load_bondret(data_dir=DATA_DIR)\n",
    "    merged_df = data.all_trace_data_merge(df_daily, df_bondret,\n",
    "                                          start_date = start_date, end_date = end_date)\n",
    "    merged_df = data.sample_selection(merged_df, start_date = start_date,\n",
    "                                      end_date = end_date)\n",
    "    \n",
    "    # Clean data\n",
    "    merged_df = merged_df.dropna(subset=['prclean'])\n",
    "    merged_df = merged_df.sort_values(by='trd_exctn_dt')\n",
    "    merged_df['month_year'] = pd.to_datetime(merged_df['trd_exctn_dt']).dt.to_period('M') \n",
    "\n",
    "    # Lags days for day_counts\n",
    "    merged_df['trd_exctn_dt_lag'] = merged_df.\\\n",
    "        groupby('cusip')['trd_exctn_dt'].shift(1)\n",
    "    dfDC = merged_df.dropna(subset=['trd_exctn_dt_lag'])\n",
    "\n",
    "    # Generate a list of U.S. holidays over this period\n",
    "    # Only include \"daily\" return if the gap between trades is less than 1-Week \n",
    "    calendar = USFederalHolidayCalendar()\n",
    "    holidays = calendar.holidays(start_date, end_date)  # 01JUL2002  # 31DEC2022\n",
    "    holiday_date_list = holidays.date.tolist()\n",
    "\n",
    "    dfDC['n']  = np.busday_count(dfDC['trd_exctn_dt_lag'].values.astype('M8[D]') , \n",
    "                                        dfDC['trd_exctn_dt'].values.astype('M8[D]'),\n",
    "                                        holidays = holiday_date_list)\n",
    "\n",
    "    df = merged_df.merge(dfDC[['cusip', 'trd_exctn_dt', 'n']],\n",
    "                         left_on = ['cusip','trd_exctn_dt'], \n",
    "                         right_on = ['cusip','trd_exctn_dt'], how = \"left\")\n",
    "    del(dfDC)\n",
    "    df = df[df.n <= 7]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean_merged_data('2003-04-14', '2009-06-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deltaprc(df):\n",
    "    \"\"\"Calculate delta price and delta price_lag for each daily trades with additional cleaning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate lagged and lead log prices, and corresponding delta p (percentage returns)\n",
    "    df['logprc']     = np.log(df['prclean'])\n",
    "    df['logprc_lag'] = df.groupby( 'cusip' )['logprc'].shift(1)\n",
    "    df['deltap']     = df ['logprc'] - df ['logprc_lag']\n",
    "\n",
    "    # Restrict log returns to be in the interval [1,1]\n",
    "    df['deltap'] = np.where(df['deltap'] > 1, 1, df['deltap'])\n",
    "    df['deltap'] = np.where(df['deltap'] <-1, -1, df['deltap'])\n",
    "\n",
    "    # Convert deltap to % i.e. returns in % as opposed to decimals\n",
    "    df['deltap'] = df['deltap'] * 100\n",
    "    \n",
    "    df['logprc_lead'] = df.groupby( 'cusip' )['logprc'].shift(-1)\n",
    "    df['deltap_lag'] = df ['logprc_lead'] - df ['logprc']\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] > 1, 1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = np.where(df['deltap_lag'] <-1, -1, df['deltap_lag'])\n",
    "    df['deltap_lag'] = df['deltap_lag'] * 100\n",
    "\n",
    "    # df.isna().sum()\n",
    "    # df_bondret.columns\n",
    "    \n",
    "    # Drop NAs in deltap, deltap_lag and bonds < 10 observations of the paired price changes\n",
    "    df_final = df.dropna(subset=['deltap', 'deltap_lag', 'prclean'])  # 'offering_date', 'price_ldm', 'offering_price', 'amount_outstanding'])\n",
    "    df_final['trade_counts'] = df_final.groupby(['cusip', 'year'])['deltap'].transform(\"count\")\n",
    "    df_final = df_final[df_final['trade_counts'] >= 10]    \n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calc_deltaprc(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_annual_illiquidity_table_daily(df):\n",
    "    \"\"\"Calculate illiquidity = -cov(deltap, deltap_lag) using daily data, groupby in month,\n",
    "    present as annual mean, median, percetage t >= 1.96, and robust t-stat. \n",
    "    \"\"\"\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    Illiq_month = df.groupby(['cusip','month_year'] )[['deltap','deltap_lag']]\\\n",
    "        .progress_apply(lambda x: x.cov().iloc[0,1]) * -1\n",
    "    Illiq_month = Illiq_month.reset_index()\n",
    "    Illiq_month.columns = ['cusip','month_year','illiq']\n",
    "    Illiq_month['year'] = Illiq_month['month_year'].dt.year\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq'])\n",
    "    # Illiq_month = Illiq_month[Illiq_month['illiq'] < 2000]\n",
    "\n",
    "    overall_illiq_mean = np.mean(Illiq_month['illiq'])\n",
    "    overall_illiq_median = Illiq_month['illiq'].median()\n",
    "\n",
    "    # Calculate t-statistics for each cusip in each year\n",
    "    Illiq_month['t_stat'] = Illiq_month.groupby(['cusip', 'year'])['illiq'].transform(\n",
    "        lambda x: (x.mean() / x.sem()) if x.sem() > 0 else np.nan)\n",
    "\n",
    "    # Identify the entries with t-stat >= 1.96 and calculate the percentage of significant t-stats for each year\n",
    "    Illiq_month['significant'] = Illiq_month['t_stat'] >= 1.96\n",
    "    percent_significant = Illiq_month.groupby('year')['significant'].mean() * 100\n",
    "    Illiq_month = Illiq_month.dropna(subset=['illiq', 't_stat'])\n",
    "    overall_percent_significant = Illiq_month['significant'].mean() * 100\n",
    "    \n",
    "    # Calculate robust t-stat for each year\n",
    "    def get_robust_t_stat(group):\n",
    "        \"\"\"Run OLS on a constant term only (mean of illiq) to get the intercept's t-stat.\"\"\"\n",
    "        X = add_constant(group['illiq'])\n",
    "        ols_result = OLS(group['illiq'], X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "\n",
    "    robust_t_stats = Illiq_month.groupby('year').apply(get_robust_t_stat)\n",
    "    \n",
    "    \n",
    "    def calculate_overall_robust_t_stat(series):\n",
    "        X = add_constant(series)\n",
    "        ols_result = OLS(series, X).fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "        return abs(ols_result.tvalues[0])\n",
    "\n",
    "    # Call the function and assign the result to overall_robust_t_stat\n",
    "    overall_robust_t_stat = calculate_overall_robust_t_stat(Illiq_month['illiq'].dropna())\n",
    "\n",
    "    # Combine the results\n",
    "    table2_daily = pd.DataFrame({\n",
    "        'Year': robust_t_stats.index,\n",
    "        'Mean_illiq': Illiq_month.groupby('year')['illiq'].mean(),\n",
    "        'Median_illiq': Illiq_month.groupby('year')['illiq'].median(),\n",
    "        'Per_t_greater_1_96': percent_significant,\n",
    "        'Robust_t_stat': robust_t_stats.values\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean_illiq': [overall_illiq_mean],\n",
    "        'Median_illiq': [overall_illiq_median],\n",
    "        'Per_t_greater_1_96': [overall_percent_significant],\n",
    "        'Robust_t_stat': [overall_robust_t_stat]\n",
    "    })\n",
    "\n",
    "    table2_daily = pd.concat([table2_daily, overall_data], ignore_index=True)\n",
    "\n",
    "    return table2_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_daily = calc_annual_illiquidity_table_daily(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annual_illiquidity_table_spd(df):\n",
    "    \"\"\"Calculate mean and median gamma implied by quoted bid-ask spreads by year.\n",
    "    \"\"\"\n",
    "    df_unique = df.groupby(['cusip', 'month_year'])['t_spread'].first().reset_index()\n",
    "    df_unique['year'] = df_unique['month_year'].dt.year  \n",
    "    df_unique = df_unique.sort_values(by='month_year')\n",
    "\n",
    "    Illiq_mean_table = df_unique.groupby('year')['t_spread'].mean()\n",
    "    overall_illiq_mean = df_unique['t_spread'].mean()\n",
    "    overall_illiq_median = df_unique['t_spread'].median()\n",
    "    \n",
    "    table2_spd = pd.DataFrame({\n",
    "        'Year': Illiq_mean_table.index,\n",
    "        'Mean implied gamma': df_unique.groupby('year')['t_spread'].mean(),\n",
    "        'Median implied gamma': df_unique.groupby('year')['t_spread'].median(),\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    overall_data = pd.DataFrame({\n",
    "        'Year': ['Full'],\n",
    "        'Mean implied gamma': [overall_illiq_mean],\n",
    "        'Median implied gamma': [overall_illiq_median]\n",
    "    })\n",
    "    \n",
    "    table2_spd = pd.concat([table2_spd, overall_data], ignore_index=True)\n",
    "    \n",
    "    return table2_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_final_spd = calc_annual_illiquidity_table_spd(df) \n",
    "# by multiplying these values by 5, we get approximately the same result as the one in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
