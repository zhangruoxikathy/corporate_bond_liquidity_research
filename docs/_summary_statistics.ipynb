{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This note book will go through the detail about how to get the table 1 result in The Illiquidity of Corporate Bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config\n",
    "import load_wrds_bondret\n",
    "import load_opensource\n",
    "import data_processing\n",
    "\n",
    "OUTPUT_DIR = config.OUTPUT_DIR\n",
    "DATA_DIR = config.DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data of Monthly, Daily and Intraday\n",
    "\n",
    "The original loading processing is very long, in the following code, we could directly load the data from our pulled directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading raw data \n",
    "# df_bondret = load_wrds_bondret.load_bondret(data_dir = DATA_DIR)\n",
    "# df_daily = load_opensource.load_daily_bond(data_dir=DATA_DIR)\n",
    "\n",
    "# loading raw data, since the data is too large, we can load the parquet file directly\n",
    "# in final case, we can comment the following two lines and use the above two lines to load the raw data\n",
    "df_bondret = pd.read_parquet(DATA_DIR / \"pulled\" / \"Bondret.parquet\")\n",
    "df_daily = pd.read_csv('/Users/adair/Desktop/FinancialTool/Group_Project/BondDailyPublic.csv')\n",
    "df_intraday = pd.read_parquet(DATA_DIR / \"pulled\" / \"intraday_clean_v2.parquet\")\n",
    "\n",
    "# pre-processing the data\n",
    "df_all = data_processing.all_trace_data_merge(df_daily, df_bondret)   #this is the dataset for panel B in table 1 \n",
    "df_sample = data_processing.sample_selection(df_all) # this is the dataset for panel A in table 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>date</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>bond_sym_id</th>\n",
       "      <th>price_eom</th>\n",
       "      <th>price_ldm</th>\n",
       "      <th>price_l5m</th>\n",
       "      <th>bsym</th>\n",
       "      <th>isin</th>\n",
       "      <th>company_symbol</th>\n",
       "      <th>...</th>\n",
       "      <th>amount_outstanding</th>\n",
       "      <th>r_sp</th>\n",
       "      <th>r_mr</th>\n",
       "      <th>r_fr</th>\n",
       "      <th>n_sp</th>\n",
       "      <th>n_mr</th>\n",
       "      <th>n_fr</th>\n",
       "      <th>rating_num</th>\n",
       "      <th>year</th>\n",
       "      <th>month_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-07-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AIR.GA</td>\n",
       "      <td>102.7910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>US000361AB18</td>\n",
       "      <td>AIR</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BAA3</td>\n",
       "      <td>BB+</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-08-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AIR.GA</td>\n",
       "      <td>103.0890</td>\n",
       "      <td>103.0890</td>\n",
       "      <td>103.0890</td>\n",
       "      <td>None</td>\n",
       "      <td>US000361AB18</td>\n",
       "      <td>AIR</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BAA3</td>\n",
       "      <td>BB+</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-09-30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AIR.GA</td>\n",
       "      <td>103.1430</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>US000361AB18</td>\n",
       "      <td>AIR</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BAA3</td>\n",
       "      <td>BB+</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-11-30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AIR.GA</td>\n",
       "      <td>102.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>US000361AB18</td>\n",
       "      <td>AIR</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BAA3</td>\n",
       "      <td>BB+</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>AIR.GA</td>\n",
       "      <td>101.9525</td>\n",
       "      <td>101.9525</td>\n",
       "      <td>101.9525</td>\n",
       "      <td>None</td>\n",
       "      <td>US000361AB18</td>\n",
       "      <td>AIR</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>BAA3</td>\n",
       "      <td>BB+</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cusip       date  issue_id bond_sym_id  price_eom  price_ldm  \\\n",
       "0  000361AB1 2002-07-31       2.0      AIR.GA   102.7910        NaN   \n",
       "1  000361AB1 2002-08-31       2.0      AIR.GA   103.0890   103.0890   \n",
       "2  000361AB1 2002-09-30       2.0      AIR.GA   103.1430        NaN   \n",
       "3  000361AB1 2002-11-30       2.0      AIR.GA   102.7500        NaN   \n",
       "4  000361AB1 2002-12-31       2.0      AIR.GA   101.9525   101.9525   \n",
       "\n",
       "   price_l5m  bsym          isin company_symbol  ... amount_outstanding r_sp  \\\n",
       "0        NaN  None  US000361AB18            AIR  ...            50000.0  BBB   \n",
       "1   103.0890  None  US000361AB18            AIR  ...            50000.0  BBB   \n",
       "2        NaN  None  US000361AB18            AIR  ...            50000.0  BBB   \n",
       "3        NaN  None  US000361AB18            AIR  ...            50000.0  BBB   \n",
       "4   101.9525  None  US000361AB18            AIR  ...            50000.0  BBB   \n",
       "\n",
       "   r_mr r_fr n_sp  n_mr  n_fr rating_num  year month_time  \n",
       "0  BAA3  BB+  9.0  10.0  11.0        9.0  2002    2002-07  \n",
       "1  BAA3  BB+  9.0  10.0  11.0        9.0  2002    2002-08  \n",
       "2  BAA3  BB+  9.0  10.0  11.0        9.0  2002    2002-09  \n",
       "3  BAA3  BB+  9.0  10.0  11.0        9.0  2002    2002-11  \n",
       "4  BAA3  BB+  9.0  10.0  11.0        9.0  2002    2002-12  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bondret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cusip_id</th>\n",
       "      <th>trd_exctn_dt</th>\n",
       "      <th>prclean</th>\n",
       "      <th>prfull</th>\n",
       "      <th>acclast</th>\n",
       "      <th>accpmt</th>\n",
       "      <th>accall</th>\n",
       "      <th>ytm</th>\n",
       "      <th>ytmt</th>\n",
       "      <th>qvolume</th>\n",
       "      <th>dvolume</th>\n",
       "      <th>coupon</th>\n",
       "      <th>mod_dur</th>\n",
       "      <th>convexity</th>\n",
       "      <th>cs_dur</th>\n",
       "      <th>cs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-08-13</td>\n",
       "      <td>100.000</td>\n",
       "      <td>102.416667</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>61.625</td>\n",
       "      <td>64.041667</td>\n",
       "      <td>0.072370</td>\n",
       "      <td>0.072370</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1.075880</td>\n",
       "      <td>1.714606</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.054854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-08-30</td>\n",
       "      <td>103.089</td>\n",
       "      <td>105.888306</td>\n",
       "      <td>2.799306</td>\n",
       "      <td>61.625</td>\n",
       "      <td>64.424306</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>0.043721</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10309.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1.040410</td>\n",
       "      <td>1.629670</td>\n",
       "      <td>0.026160</td>\n",
       "      <td>0.025820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-09-06</td>\n",
       "      <td>103.143</td>\n",
       "      <td>106.063139</td>\n",
       "      <td>2.920139</td>\n",
       "      <td>61.625</td>\n",
       "      <td>64.545139</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>51572.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1.024589</td>\n",
       "      <td>1.589488</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>0.024964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-11-08</td>\n",
       "      <td>83.575</td>\n",
       "      <td>84.118750</td>\n",
       "      <td>0.543750</td>\n",
       "      <td>65.250</td>\n",
       "      <td>65.793750</td>\n",
       "      <td>0.286619</td>\n",
       "      <td>0.286619</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>835750.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.792236</td>\n",
       "      <td>0.981177</td>\n",
       "      <td>0.272319</td>\n",
       "      <td>0.272319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>000361AB1</td>\n",
       "      <td>2002-12-02</td>\n",
       "      <td>99.000</td>\n",
       "      <td>99.986806</td>\n",
       "      <td>0.986806</td>\n",
       "      <td>65.250</td>\n",
       "      <td>66.236806</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>14850.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.811968</td>\n",
       "      <td>1.056617</td>\n",
       "      <td>0.068965</td>\n",
       "      <td>0.068965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   cusip_id trd_exctn_dt  prclean      prfull   acclast  accpmt  \\\n",
       "0           0  000361AB1   2002-08-13  100.000  102.416667  2.416667  61.625   \n",
       "1           1  000361AB1   2002-08-30  103.089  105.888306  2.799306  61.625   \n",
       "2           2  000361AB1   2002-09-06  103.143  106.063139  2.920139  61.625   \n",
       "3           3  000361AB1   2002-11-08   83.575   84.118750  0.543750  65.250   \n",
       "4           4  000361AB1   2002-12-02   99.000   99.986806  0.986806  65.250   \n",
       "\n",
       "      accall       ytm      ytmt    qvolume   dvolume  coupon   mod_dur  \\\n",
       "0  64.041667  0.072370  0.072370    25000.0   25000.0    7.25  1.075880   \n",
       "1  64.424306  0.043721  0.043721    10000.0   10309.0    7.25  1.040410   \n",
       "2  64.545139  0.042815  0.042815    50000.0   51572.0    7.25  1.024589   \n",
       "3  65.793750  0.286619  0.286619  1000000.0  835750.0    7.25  0.792236   \n",
       "4  66.236806  0.084565  0.084565    15000.0   14850.0    7.25  0.811968   \n",
       "\n",
       "   convexity    cs_dur        cs  \n",
       "0   1.714606  0.055142  0.054854  \n",
       "1   1.629670  0.026160  0.025820  \n",
       "2   1.589488  0.025233  0.024964  \n",
       "3   0.981177  0.272319  0.272319  \n",
       "4   1.056617  0.068965  0.068965  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cusip</th>\n",
       "      <th>bond_sym_id</th>\n",
       "      <th>trd_exctn_dt</th>\n",
       "      <th>trd_exctn_tm</th>\n",
       "      <th>days_to_sttl_ct</th>\n",
       "      <th>lckd_in_ind</th>\n",
       "      <th>wis_fl</th>\n",
       "      <th>sale_cndtn_cd</th>\n",
       "      <th>msg_seq_nb</th>\n",
       "      <th>...</th>\n",
       "      <th>trd_rpt_tm</th>\n",
       "      <th>entrd_vol_qt</th>\n",
       "      <th>rptd_pr</th>\n",
       "      <th>yld_pt</th>\n",
       "      <th>asof_cd</th>\n",
       "      <th>orig_msg_seq_nb</th>\n",
       "      <th>rpt_side_cd</th>\n",
       "      <th>cntra_mp_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85110</td>\n",
       "      <td>001957AP4</td>\n",
       "      <td>T.GE</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>08:08:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>@</td>\n",
       "      <td>772</td>\n",
       "      <td>...</td>\n",
       "      <td>08:36:28</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>107.867</td>\n",
       "      <td>4.756000</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>D</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85111</td>\n",
       "      <td>001957AP4</td>\n",
       "      <td>T.GE</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>08:08:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>@</td>\n",
       "      <td>773</td>\n",
       "      <td>...</td>\n",
       "      <td>08:36:32</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>108.242</td>\n",
       "      <td>4.632000</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85112</td>\n",
       "      <td>001957AP4</td>\n",
       "      <td>T.GE</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>10:55:34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>@</td>\n",
       "      <td>5885</td>\n",
       "      <td>...</td>\n",
       "      <td>10:55:45</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>106.250</td>\n",
       "      <td>5.298545</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85113</td>\n",
       "      <td>001957AP4</td>\n",
       "      <td>T.GE</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>11:45:11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>@</td>\n",
       "      <td>9187</td>\n",
       "      <td>...</td>\n",
       "      <td>11:45:18</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>108.212</td>\n",
       "      <td>4.642000</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85114</td>\n",
       "      <td>001957AP4</td>\n",
       "      <td>T.GE</td>\n",
       "      <td>2003-04-14</td>\n",
       "      <td>11:58:46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>@</td>\n",
       "      <td>10290</td>\n",
       "      <td>...</td>\n",
       "      <td>11:59:05</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>107.500</td>\n",
       "      <td>4.878000</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cusip bond_sym_id trd_exctn_dt trd_exctn_tm  \\\n",
       "0       85110  001957AP4        T.GE   2003-04-14     08:08:45   \n",
       "1       85111  001957AP4        T.GE   2003-04-14     08:08:45   \n",
       "2       85112  001957AP4        T.GE   2003-04-14     10:55:34   \n",
       "3       85113  001957AP4        T.GE   2003-04-14     11:45:11   \n",
       "4       85114  001957AP4        T.GE   2003-04-14     11:58:46   \n",
       "\n",
       "   days_to_sttl_ct  lckd_in_ind wis_fl sale_cndtn_cd  msg_seq_nb  ...  \\\n",
       "0              0.0          NaN      N             @         772  ...   \n",
       "1              0.0          NaN      N             @         773  ...   \n",
       "2              0.0          NaN      N             @        5885  ...   \n",
       "3              0.0          NaN      N             @        9187  ...   \n",
       "4              0.0          NaN      N             @       10290  ...   \n",
       "\n",
       "  trd_rpt_tm entrd_vol_qt  rptd_pr    yld_pt  asof_cd  orig_msg_seq_nb  \\\n",
       "0   08:36:28       5000.0  107.867  4.756000     None              NaN   \n",
       "1   08:36:32       5000.0  108.242  4.632000     None              NaN   \n",
       "2   10:55:45       5000.0  106.250  5.298545     None              NaN   \n",
       "3   11:45:18      10000.0  108.212  4.642000     None              NaN   \n",
       "4   11:59:05       1000.0  107.500  4.878000     None              NaN   \n",
       "\n",
       "  rpt_side_cd  cntra_mp_id  year month  \n",
       "0           S            D  2003     4  \n",
       "1           S            C  2003     4  \n",
       "2           B            D  2003     4  \n",
       "3           S            C  2003     4  \n",
       "4           S            C  2003     4  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_intraday.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Data manipulation and merging of different dataframes.\n",
    "\n",
    "The first part of the code is adding a new column 'month' to the dataframes `df_all` and `df_sample`. This is done by extracting the month from the 'date' column using the `dt.month` attribute. \n",
    "\n",
    "The next part of the code is preparing the `df_intraday` dataframe for merging. The 'cusip_id' column is renamed to 'cusip' to match the other dataframes. The 'trd_exctn_dt' column, which appears to represent the date of trade execution, is converted to datetime format. Then, 'year' and 'month' columns are created by extracting the year and month from the 'trd_exctn_dt' column.\n",
    "\n",
    "The `df_intraday` dataframe is then grouped by 'year', 'month', and 'cusip', and the number of trades for each group is counted. This results in a new dataframe `df_intraday_grouped` with a '#trade' column representing the count of trades.\n",
    "\n",
    "Finally, the `df_intraday_grouped` dataframe is merged with `df_sample` and `df_all` dataframes using a left join on the 'year', 'month', and 'cusip' columns. This means that all rows from `df_sample` and `df_all` and only matching rows from `df_intraday_grouped` will be included in the resulting dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give df_all and df_sample a month column\n",
    "df_all['month'] = df_all['date'].dt.month\n",
    "df_sample['month'] = df_sample['date'].dt.month\n",
    "\n",
    "# merge the df_intraday_grouped with df_sample and df_all by year, month and cusip, only keep the #trade column\n",
    "df_intraday.rename(columns={'cusip_id': 'cusip'}, inplace=True)\n",
    "df_intraday['trd_exctn_dt'] = pd.to_datetime(df_intraday['trd_exctn_dt'])\n",
    "df_intraday['year'] = df_intraday['trd_exctn_dt'].dt.year\n",
    "df_intraday['month'] = df_intraday['trd_exctn_dt'].dt.month\n",
    "df_intraday_grouped = df_intraday.groupby(['year', 'month', 'cusip'])['trd_exctn_dt'].count().reset_index(name='#trade')\n",
    "\n",
    "df_sample = pd.merge(df_sample, df_intraday_grouped, how='left', on=['year', 'month', 'cusip'])\n",
    "\n",
    "df_all = pd.merge(df_all, df_intraday_grouped, how='left', on=['year', 'month', 'cusip'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Develop helper functions\n",
    "\n",
    "These functions are designed to perform common statistical calculations on a specified column of a pandas DataFrame, grouped by year.\n",
    "\n",
    "The first function, `cal_avrage`, calculates the average (mean) of a specified column for each year. It does this by grouping the DataFrame by the 'year' column and then applying the `mean` function to the specified column. The resulting DataFrame is then reset to a default index, and the column name is renamed to include '_avg' at the end.\n",
    "\n",
    "The second function, `cal_median`, calculates the median of a specified column for each year. Similar to the first function, it groups the DataFrame by the 'year' column and applies the `median` function to the specified column. The resulting DataFrame is reset to a default index, and the column name is renamed to include '_median' at the end.\n",
    "\n",
    "The third function, `cal_std`, calculates the standard deviation of a specified column for each year. It groups the DataFrame by the 'year' column and applies the `std` function to the specified column. The resulting DataFrame is reset to a default index, and the column name is renamed to include '_std' at the end.\n",
    "\n",
    "The fourth function, `cal_count`, calculates the count of unique values in a specified column for each year. It groups the DataFrame by the 'year' column and applies the `nunique` function to the specified column. The resulting DataFrame is reset to a default index, and the column name is renamed to include '_count' at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to calculate the average, median, standard deviation and count of a column in a dataframe\n",
    "\n",
    "def cal_avrage(dataframe, column):\n",
    "    \"\"\"\n",
    "    Calculate the average of a specified column in a dataframe grouped by year.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The input dataframe.\n",
    "    column (str): The column name for which the average is calculated.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A series containing the average values for each year.\n",
    "    \"\"\"\n",
    "    average = dataframe.groupby('year')[column].mean().reset_index()\n",
    "    average.rename(columns={column: column+'_avg'}, inplace=True)\n",
    "    average.set_index('year', inplace=True)\n",
    "\n",
    "    return average\n",
    "\n",
    "def cal_median(dataframe, column):\n",
    "    \"\"\"\n",
    "    Calculate the median value of a specified column in a dataframe, grouped by year.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The input dataframe.\n",
    "    column (str): The name of the column to calculate the median for.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A series containing the median values for each year.\n",
    "    \"\"\"\n",
    "    median = dataframe.groupby('year')[column].median().reset_index()\n",
    "    median.rename(columns={column: column+'_median'}, inplace=True)\n",
    "    median.set_index('year', inplace=True)\n",
    "\n",
    "    return median\n",
    "\n",
    "def cal_std(dataframe, column):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation of a column in a dataframe grouped by year.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input dataframe.\n",
    "        column (str): The name of the column to calculate the standard deviation for.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: The standard deviation of the specified column grouped by year.\n",
    "    \"\"\"\n",
    "    std = dataframe.groupby('year')[column].std().reset_index()\n",
    "    std.rename(columns={column: column+'_std'}, inplace=True)\n",
    "    std.set_index('year', inplace=True)\n",
    "\n",
    "    return std\n",
    "\n",
    "def cal_count(dataframe, column='cusip'):\n",
    "    \"\"\"\n",
    "    Calculate the count of unique values in a specified column of a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The input dataframe.\n",
    "    column (str): The column name to calculate the count of unique values. Default is 'cusip'.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: A series containing the count of unique values for each year.\n",
    "    \"\"\"\n",
    "    count = dataframe.groupby('year')[column].nunique().reset_index()\n",
    "    count.rename(columns={column: column+'_count'}, inplace=True)\n",
    "    count.set_index('year', inplace=True)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate various statistics (No need to drop month;y duplicate)\n",
    "The following code is primarily concerned with calculating various statistics for different attributes of two dataframes: `df_sample` and `df_all`.\n",
    "\n",
    "The first part of the code calculates the number of unique 'cusip' values in both dataframes using the `cal_count` function. 'Cusip' is a term used in finance that stands for Committee on Uniform Securities Identification Procedures. It's a unique identifier for bonds and other financial instruments.\n",
    "\n",
    "Next, the code calculates the 'issuance' for both dataframes. The 'issuance' is calculated as the product of 'offering_amt', 'principal_amt', and 'offering_price', divided by 100 (get precentage) and then by 1,000,000(unit: million). The average, median, and standard deviation of the 'issuance' are then calculated using the `cal_avrage`, `cal_median`, and `cal_std` functions respectively.\n",
    "\n",
    "The code then repeats this process for several other attributes: 'n_mr' (Moody's Rating), 'tmt' (Maturity), 'coupon_y' (Coupon), 'age', and 'turnover'. For 'age', the code first converts the 'date' and 'offering_date' columns to datetime format, then calculates 'age' as the difference between these two dates in years. For 'turnover', it's calculated as 't_volume' divided by 'issuance' and then divided by 10,000.\n",
    "\n",
    "The resulting statistics for each attribute are stored in separate dataframes (e.g., `df_sample_issuance`, `df_all_moody`, `df_sample_turnover`, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique cusips in df_sample and df_all\n",
    "df_sample_cusip = cal_count(df_sample)\n",
    "df_all_cusip = cal_count(df_all)\n",
    "\n",
    "\n",
    "# Calculate the Issuance of df_sample and df_all\n",
    "df_sample['issuance'] = df_sample['offering_amt'] * df_sample['principal_amt'] * \\\n",
    "                        df_sample['offering_price'] / 100 / 1000000\n",
    "\n",
    "df_sample_issuance = pd.concat([cal_avrage(df_sample, 'issuance'), \\\n",
    "                    cal_median(df_sample, 'issuance'), cal_std(df_sample, 'issuance')], axis=1)\n",
    "\n",
    "df_all['issuance'] = df_all['offering_amt'] * df_all['principal_amt'] * \\\n",
    "                     df_all['offering_price'] / 100 / 1000000\n",
    "\n",
    "df_all_issuance = pd.concat([cal_avrage(df_all, 'issuance'), \\\n",
    "                    cal_median(df_all, 'issuance'), cal_std(df_all, 'issuance')], axis=1)\n",
    "\n",
    "# Calculate the Moondy Rating of df_sample and df_all\n",
    "df_sample_moody = pd.concat([cal_avrage(df_sample, 'n_mr'), \\\n",
    "                    cal_median(df_sample, 'n_mr'), cal_std(df_sample, 'n_mr')], axis=1)\n",
    "\n",
    "df_all_moody = pd.concat([cal_avrage(df_all, 'n_mr'), \\\n",
    "                    cal_median(df_all, 'n_mr'), cal_std(df_all, 'n_mr')], axis=1)\n",
    "\n",
    "# Calculate the Maturity of df_sample and df_all\n",
    "df_sample_maturity = pd.concat([cal_avrage(df_sample, 'tmt'), \\\n",
    "                    cal_median(df_sample, 'tmt'), cal_std(df_sample, 'tmt')], axis=1)\n",
    "\n",
    "df_all_maturity = pd.concat([cal_avrage(df_all, 'tmt'), \\\n",
    "                    cal_median(df_all, 'tmt'), cal_std(df_all, 'tmt')], axis=1)\n",
    "\n",
    "# Calculate the coupon of df_sample and df_all\n",
    "df_sample_coupon = pd.concat([cal_avrage(df_sample, 'coupon_y'), \\\n",
    "                    cal_median(df_sample, 'coupon_y'), cal_std(df_sample, 'coupon_y')], axis=1)\n",
    "\n",
    "df_all_coupon = pd.concat([cal_avrage(df_all, 'coupon_y'), \\\n",
    "                    cal_median(df_all, 'coupon_y'), cal_std(df_all, 'coupon_y')], axis=1)\n",
    "\n",
    "# Calculate the age where the gap between the issuance date and the trade date in years\n",
    "df_sample[['date', 'offering_date']] = df_sample[['date', 'offering_date']].apply(pd.to_datetime)\n",
    "df_all[['date', 'offering_date']] = df_all[['date', 'offering_date']].apply(pd.to_datetime)\n",
    "\n",
    "df_sample['age'] = (df_sample['date'] - df_sample['offering_date']).dt.days / 365\n",
    "df_all['age'] = (df_all['date'] - df_all['offering_date']).dt.days / 365\n",
    "\n",
    "df_sample_age = pd.concat([cal_avrage(df_sample, 'age'), \\\n",
    "                    cal_median(df_sample, 'age'), cal_std(df_sample, 'age')], axis=1)\n",
    "\n",
    "df_all_age = pd.concat([cal_avrage(df_all, 'age'), \\\n",
    "                    cal_median(df_all, 'age'), cal_std(df_all, 'age')], axis=1)\n",
    "\n",
    "# Calculate the turnover in df_sample and df_all\n",
    "df_sample['turnover'] = df_sample['t_volume'] / df_sample['issuance'] / 10000\n",
    "df_all['turnover'] = df_all['t_volume'] / df_all['issuance'] / 10000\n",
    "\n",
    "df_sample_turnover = pd.concat([cal_avrage(df_sample, 'turnover'), \\\n",
    "                    cal_median(df_sample, 'turnover'), cal_std(df_sample, 'turnover')], axis=1)\n",
    "\n",
    "df_all_turnover = pd.concat([cal_avrage(df_all, 'turnover'), \\\n",
    "                    cal_median(df_all, 'turnover'), cal_std(df_all, 'turnover')], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate various statistics (Need to drop month;y duplicate)\n",
    "The following code is primarily concerned with calculating various statistics that need to first drop monthly duplicate for different attributes of two dataframes: `df_sample` and `df_all`.\n",
    "\n",
    "The first part of the code calculates the monthly return for both dataframes. It first removes duplicate entries based on 'cusip', 'year', and 'month' columns. Then, it calculates the 'return' as the logarithm of the ratio of the current month's 'price_eom' (end of month price) to the previous month's 'price_eom', multiplied by 100. This is done separately for each 'cusip' using the `groupby` function and the `shift` function.\n",
    "\n",
    "Next, the code groups the dataframes by 'year' and 'cusip' and calculates the average return for each group. The resulting dataframes are then passed to the `cal_avrage`, `cal_median`, and `cal_std` functions to calculate the average, median, and standard deviation of the average return, respectively.\n",
    "\n",
    "The code then calculates the volatility of the return for both dataframes. The volatility is calculated as the standard deviation of the 'return' for each group of 'year' and 'cusip'. The resulting dataframes are then passed to the `cal_avrage`, `cal_median`, and `cal_std` functions to calculate the average, median, and standard deviation of the volatility, respectively.\n",
    "\n",
    "The code then calculates the average, median, and standard deviation of the 'prclean' (clean price) for both dataframes. This is done by grouping the dataframes by 'year', 'cusip', and 'date', and calculating the mean 'prclean' for each group.\n",
    "\n",
    "The code then calculates the average, median, and standard deviation of the number of trades ('#trade') for both dataframes.\n",
    "\n",
    "Finally, the code calculates the 'trade_size' as the ratio of 't_dvolume' (daily trading volume) to '#trade', divided by 1000. The average, median, and standard deviation of the 'trade_size' are then calculated for both dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the return of df_sample and df_all\n",
    "\n",
    "# We need to drop teh duplicate entires in df_sample and df_all\n",
    "df_sample_month = df_sample.drop_duplicates(subset=['cusip', 'year', 'month']).reset_index()\n",
    "df_all_month = df_all.drop_duplicates(subset=['cusip', 'year', 'month']).reset_index()\n",
    "\n",
    "df_sample_month['return'] = np.log(df_sample_month['price_eom'] / \\\n",
    "                            df_sample_month.groupby(['cusip'])['price_eom'].shift(1)) * 100\n",
    "\n",
    "df_all_month['return'] = np.log(df_all_month['price_eom'] / \\\n",
    "                            df_all_month.groupby(['cusip'])['price_eom'].shift(1)) * 100\n",
    "\n",
    "\n",
    "# group by year and cusip and calculate the weighted average return\n",
    "df_sample_month_grouped = df_sample_month.groupby(['year', 'cusip'])['return'].mean().reset_index(name='Avg_return')\n",
    "df_all_month_grouped = df_all_month.groupby(['year', 'cusip'])['return'].mean().reset_index(name='Avg_return')\n",
    "\n",
    "\n",
    "df_sample_return = pd.concat([cal_avrage(df_sample_month_grouped, 'Avg_return'), \\\n",
    "                    cal_median(df_sample_month_grouped, 'Avg_return'), cal_std(df_sample_month_grouped, 'Avg_return')], axis=1)\n",
    "\n",
    "df_all_return = pd.concat([cal_avrage(df_all_month_grouped, 'Avg_return'), \\\n",
    "                    cal_median(df_all_month_grouped, 'Avg_return'), cal_std(df_all_month_grouped, 'Avg_return')], axis=1)\n",
    "\n",
    "# Calculate the volatility of df_sample and df_all\n",
    "\n",
    "df_sample_vol_grouped = df_sample_month.groupby(['year', 'cusip'])['return'].std().reset_index(name='volatility')\n",
    "df_all_vol_grouped = df_all_month.groupby(['year', 'cusip'])['return'].std().reset_index(name='volatility')\n",
    "\n",
    "df_sample_vol = pd.concat([cal_avrage(df_sample_vol_grouped, 'volatility'), \\\n",
    "                    cal_median(df_sample_vol_grouped, 'volatility'), cal_std(df_sample_vol_grouped, 'volatility')], axis=1)\n",
    "\n",
    "df_all_vol = pd.concat([cal_avrage(df_all_vol_grouped, 'volatility'), \\\n",
    "                    cal_median(df_all_vol_grouped, 'volatility'), cal_std(df_all_vol_grouped, 'volatility')], axis=1)\n",
    "\n",
    "# Calculate the Price in df_sample and df_all\n",
    "df_sample_month_price = df_sample.groupby(['year', 'cusip', 'date'])['prclean'].mean().reset_index()\n",
    "df_all_month_price = df_all.groupby(['year', 'cusip', 'date'])['prclean'].mean().reset_index()\n",
    "\n",
    "df_sample_price = pd.concat([cal_avrage(df_sample_month_price, 'prclean'), \\\n",
    "                    cal_median(df_sample_month_price, 'prclean'), cal_std(df_sample_month_price, 'prclean')], axis=1)\n",
    "\n",
    "df_all_price = pd.concat([cal_avrage(df_all_month_price, 'prclean'), \\\n",
    "                    cal_median(df_all_month_price, 'prclean'), cal_std(df_all_month_price, 'prclean')], axis=1)\n",
    "\n",
    "# Calculate the number of trades in df_sample and df_all\n",
    "df_sample_trade = pd.concat([cal_avrage(df_sample_month, '#trade'), \\\n",
    "                    cal_median(df_sample_month, '#trade'), cal_std(df_sample_month, '#trade')], axis=1)\n",
    "\n",
    "df_all_trade = pd.concat([cal_avrage(df_all_month, '#trade'), \\\n",
    "                    cal_median(df_all_month, '#trade'), cal_std(df_all_month, '#trade')], axis=1)\n",
    "\n",
    "\n",
    "# Calculate the Trade_size in df_sample and df_all\n",
    "df_sample_month['trade_size'] = df_sample_month['t_dvolume'] / df_sample_month['#trade'] / 1000\n",
    "df_all_month['trade_size'] = df_all_month['t_dvolume'] / df_all_month['#trade'] / 1000\n",
    "\n",
    "df_sample_size = pd.concat([cal_avrage(df_sample_month, 'trade_size'), \\\n",
    "                    cal_median(df_sample_month, 'trade_size'), cal_std(df_sample_month, 'trade_size')], axis=1)\n",
    "\n",
    "df_all_size = pd.concat([cal_avrage(df_all_month, 'trade_size'), \\\n",
    "                    cal_median(df_all_month, 'trade_size'), cal_std(df_all_month, 'trade_size')], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Combine the result\n",
    "The following code is primarily concerned with consolidating the results of various statistical calculations into two final dataframes: `df_sample_result` and `df_all_result`.\n",
    "\n",
    "The first part of the code concatenates multiple dataframes (`df_sample_cusip`, `df_sample_issuance`, `df_sample_moody`, etc.) along the column axis (axis=1) using the `pd.concat` function. This results in two new dataframes `df_sample_result` and `df_all_result` where each column represents the results of a specific statistical calculation for the 'sample' and 'all' datasets respectively.\n",
    "\n",
    "The second part of the code transposes these two dataframes using the `.T` attribute. Transposing a dataframe swaps its rows and columns. After this operation, each row in `df_sample_result` and `df_all_result` represents the results of a specific statistical calculation, and the columns represent the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all results of df_sample and df_all\n",
    "df_sample_result = pd.concat([df_sample_cusip, df_sample_issuance, df_sample_moody, df_sample_maturity, df_sample_coupon, \\\n",
    "                    df_sample_age, df_sample_turnover, df_sample_size, df_sample_trade, df_sample_return, df_sample_vol, df_sample_price], axis=1)\n",
    "\n",
    "df_all_result = pd.concat([df_all_cusip, df_all_issuance, df_all_moody, df_all_maturity, df_all_coupon, \\\n",
    "                    df_all_age, df_all_turnover, df_all_size, df_all_trade, df_all_return, df_all_vol, df_all_price], axis=1)\n",
    "# transform the df_sample_result, make its index as column\n",
    "df_sample_result = df_sample_result.T\n",
    "df_all_result = df_all_result.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cusip_count</th>\n",
       "      <td>781.000000</td>\n",
       "      <td>896.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>513.000000</td>\n",
       "      <td>426.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issuance_avg</th>\n",
       "      <td>992.244968</td>\n",
       "      <td>981.885959</td>\n",
       "      <td>990.048752</td>\n",
       "      <td>983.184266</td>\n",
       "      <td>1001.462032</td>\n",
       "      <td>1031.990589</td>\n",
       "      <td>1070.582242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issuance_median</th>\n",
       "      <td>749.460000</td>\n",
       "      <td>749.355000</td>\n",
       "      <td>771.829290</td>\n",
       "      <td>797.360000</td>\n",
       "      <td>797.896000</td>\n",
       "      <td>847.968500</td>\n",
       "      <td>990.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issuance_std</th>\n",
       "      <td>735.051477</td>\n",
       "      <td>712.180961</td>\n",
       "      <td>696.051084</td>\n",
       "      <td>658.993069</td>\n",
       "      <td>675.693519</td>\n",
       "      <td>705.112816</td>\n",
       "      <td>725.922416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_mr_avg</th>\n",
       "      <td>5.865775</td>\n",
       "      <td>5.672186</td>\n",
       "      <td>5.604930</td>\n",
       "      <td>5.292138</td>\n",
       "      <td>5.209142</td>\n",
       "      <td>5.592997</td>\n",
       "      <td>6.323899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year                   2003        2004        2005        2006         2007  \\\n",
       "cusip_count      781.000000  896.000000  861.000000  723.000000   611.000000   \n",
       "issuance_avg     992.244968  981.885959  990.048752  983.184266  1001.462032   \n",
       "issuance_median  749.460000  749.355000  771.829290  797.360000   797.896000   \n",
       "issuance_std     735.051477  712.180961  696.051084  658.993069   675.693519   \n",
       "n_mr_avg           5.865775    5.672186    5.604930    5.292138     5.209142   \n",
       "\n",
       "year                    2008         2009  \n",
       "cusip_count       513.000000   426.000000  \n",
       "issuance_avg     1031.990589  1070.582242  \n",
       "issuance_median   847.968500   990.480000  \n",
       "issuance_std      705.112816   725.922416  \n",
       "n_mr_avg            5.592997     6.323899  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cusip_count</th>\n",
       "      <td>14176.000000</td>\n",
       "      <td>16299.000000</td>\n",
       "      <td>16848.000000</td>\n",
       "      <td>16691.000000</td>\n",
       "      <td>16898.000000</td>\n",
       "      <td>16666.000000</td>\n",
       "      <td>13974.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issuance_avg</th>\n",
       "      <td>503.476298</td>\n",
       "      <td>539.175215</td>\n",
       "      <td>578.808925</td>\n",
       "      <td>596.444788</td>\n",
       "      <td>632.527829</td>\n",
       "      <td>714.011625</td>\n",
       "      <td>735.856447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issuance_median</th>\n",
       "      <td>299.760000</td>\n",
       "      <td>349.422500</td>\n",
       "      <td>399.284000</td>\n",
       "      <td>447.025500</td>\n",
       "      <td>496.790000</td>\n",
       "      <td>499.025000</td>\n",
       "      <td>499.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issuance_std</th>\n",
       "      <td>549.508040</td>\n",
       "      <td>560.122080</td>\n",
       "      <td>573.607099</td>\n",
       "      <td>573.267895</td>\n",
       "      <td>595.740186</td>\n",
       "      <td>688.381915</td>\n",
       "      <td>743.562726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_mr_avg</th>\n",
       "      <td>8.482803</td>\n",
       "      <td>8.508066</td>\n",
       "      <td>8.548406</td>\n",
       "      <td>8.659461</td>\n",
       "      <td>8.498669</td>\n",
       "      <td>8.491624</td>\n",
       "      <td>8.811422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year                     2003          2004          2005          2006  \\\n",
       "cusip_count      14176.000000  16299.000000  16848.000000  16691.000000   \n",
       "issuance_avg       503.476298    539.175215    578.808925    596.444788   \n",
       "issuance_median    299.760000    349.422500    399.284000    447.025500   \n",
       "issuance_std       549.508040    560.122080    573.607099    573.267895   \n",
       "n_mr_avg             8.482803      8.508066      8.548406      8.659461   \n",
       "\n",
       "year                     2007          2008          2009  \n",
       "cusip_count      16898.000000  16666.000000  13974.000000  \n",
       "issuance_avg       632.527829    714.011625    735.856447  \n",
       "issuance_median    496.790000    499.025000    499.075000  \n",
       "issuance_std       595.740186    688.381915    743.562726  \n",
       "n_mr_avg             8.498669      8.491624      8.811422  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result\n",
    "df_sample_result.to_csv(OUTPUT_DIR / 'table1_panelA.csv')\n",
    "df_all_result.to_csv(OUTPUT_DIR / 'table1_panelB.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
